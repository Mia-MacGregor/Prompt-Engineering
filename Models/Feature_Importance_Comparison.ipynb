{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a467af-4869-4b76-8a6b-e9c02f205f2a",
   "metadata": {},
   "source": [
    "# Comparison of feature importances from model generated csv's using permutation feature importance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5470a51-69c4-4a00-904e-f5a032b243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS4.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS4_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS4_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27b736-0dd7-4109-a4e9-ba40955836bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Alt1c.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS4_Alt1c.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS4_Alt1c_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS4_Alt1c_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10aabb-13a1-4431-83ca-28bc448abd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_AltB.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS4_AltB.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS4_AltB_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS4_AltB_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76fc498-c4b0-4b1e-98b3-ffd4e7579765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS3.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS3_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI+DS3_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87e6a9-e433-4b98-a2a2-9033687f28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS5.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS5_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS5_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731226c-7d0b-43b3-96da-c818d567038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS6.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS6_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS6_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479dea7-b19f-4d49-b023-674473c445d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS7.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS7_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS7_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e40b7-bf50-464f-9062-134cca62419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS8.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS8_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS8_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f12154-2cf0-4f33-9017-dd363dcdb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- Load files ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS3.csv'\n",
    "\n",
    "# Define output paths\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs'\n",
    "comparison_csv_path = os.path.join(output_dir, 'Feature_Importance_Comparison.csv')\n",
    "plot_path = os.path.join(output_dir, 'Feature_Importance_Plot.png')\n",
    "\n",
    "# --- Load and clean the ranked feature list from TXT ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load feature importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "model_names = df_importances.iloc[:, 0]\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "\n",
    "# --- Reorder columns to match .txt list (must match names exactly) ---\n",
    "df_importances = df_importances[ranked_features]\n",
    "\n",
    "# --- Compute average importance across models ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison table to CSV ---\n",
    "comparison_df.to_csv(comparison_csv_path)\n",
    "print(f\"\\n✅ Comparison table saved to:\\n{comparison_csv_path}\")\n",
    "\n",
    "# --- Plot comparison ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# --- Save plot to PNG ---\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01a221-6df3-4287-ac78-f9be75810b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Optional: for saving table image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Config: Manually specify CSV files to compare ---\n",
    "csv_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6.csv'\n",
    "]\n",
    "\n",
    "# Feature names text file\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "\n",
    "# Output directory for plot and table\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'DS4-5-6_FI_Comparison.png')\n",
    "table_path = os.path.join(output_dir, 'DS4-5-6_FI_Table.png')\n",
    "\n",
    "# --- Load and clean feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    raw_features = [line.strip() for line in file if line.strip()]\n",
    "feature_list = [re.sub(r\"^\\d+\\.\\s*\", \"\", feat) for feat in raw_features]\n",
    "\n",
    "# --- Combine all manually listed CSVs ---\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    model_col = temp_df.columns[0]  # assume first column is model name\n",
    "    temp_df.set_index(model_col, inplace=True)\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    temp_df = temp_df[feature_list]\n",
    "    \n",
    "    temp_df['Dataset'] = os.path.basename(file)  # tag source\n",
    "    combined_df = pd.concat([combined_df, temp_df], axis=0)\n",
    "\n",
    "# --- Calculate average importance across all models and datasets ---\n",
    "mean_importances = combined_df[feature_list].mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Mean Importance': mean_importances\n",
    "})\n",
    "comparison_df['Importance Rank'] = comparison_df['Mean Importance'].rank(ascending=False).astype(int)\n",
    "comparison_df = comparison_df.sort_values('Importance Rank')\n",
    "\n",
    "# --- Save table as image using matplotlib ---\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))\n",
    "ax.axis('off')\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.reset_index().values,\n",
    "    colLabels=['Feature'] + list(comparison_df.columns),\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.savefig(table_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"✅ Table saved to: {table_path}\")\n",
    "\n",
    "# --- Plot overall feature importance ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette='mako')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Overall Average Feature Importance Across All Datasets\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ Plot saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf2809-0544-4091-a42b-8f8f5f996b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Optional: for saving table image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Config: Manually specify CSV files to compare ---\n",
    "csv_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8.csv'\n",
    "]\n",
    "\n",
    "# Feature names text file\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "\n",
    "# Output directory for plot and table\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'Overall_FI_Comparison.png')\n",
    "table_path = os.path.join(output_dir, 'Overall_FI_Table.png')\n",
    "\n",
    "# --- Load and clean feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    raw_features = [line.strip() for line in file if line.strip()]\n",
    "feature_list = [re.sub(r\"^\\d+\\.\\s*\", \"\", feat) for feat in raw_features]\n",
    "\n",
    "# --- Combine all manually listed CSVs ---\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    model_col = temp_df.columns[0]  # assume first column is model name\n",
    "    temp_df.set_index(model_col, inplace=True)\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    temp_df = temp_df[feature_list]\n",
    "    \n",
    "    temp_df['Dataset'] = os.path.basename(file)  # tag source\n",
    "    combined_df = pd.concat([combined_df, temp_df], axis=0)\n",
    "\n",
    "# --- Calculate average importance across all models and datasets ---\n",
    "mean_importances = combined_df[feature_list].mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Mean Importance': mean_importances\n",
    "})\n",
    "comparison_df['Importance Rank'] = comparison_df['Mean Importance'].rank(ascending=False).astype(int)\n",
    "comparison_df = comparison_df.sort_values('Importance Rank')\n",
    "\n",
    "# --- Save table as image using matplotlib ---\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))\n",
    "ax.axis('off')\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.reset_index().values,\n",
    "    colLabels=['Feature'] + list(comparison_df.columns),\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.savefig(table_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"✅ Table saved to: {table_path}\")\n",
    "\n",
    "# --- Plot overall feature importance ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette='mako')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Overall Average Feature Importance Across All Datasets\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ Plot saved to: {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722169f-67a6-497d-a853-69a4ff95168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculated versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acba30-7911-4f23-ac38-a3ca7c8bb94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS3_calc.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS3_calc_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS3_calc_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8905a7-dddc-4424-bc23-29c275887133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS4_calc.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS4_calc_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS4_calc_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa1448-cbc2-4a58-b603-cb8ed05be830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS5_calc.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS5_calc_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS5_calc_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793319eb-289f-4f1b-8efc-820fa8a42003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS6_calc.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS6_calc_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS6_calc_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b561a17f-93db-40e4-beaf-2bbfb9454fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS7_calc.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS7_calc_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS7_calc_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c1aa2-7189-4008-8453-d2a23ec00e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import dataframe_image as dfi\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "importances_csv_path = 'Feature_Importance_DS8_calc.csv'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'FI_DS8_calc_Plot.png')\n",
    "table_image_path = os.path.join(output_dir, 'FI_DS8_calc_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    ranked_features = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# --- Load importances from CSV ---\n",
    "df_importances = pd.read_csv(importances_csv_path)\n",
    "df_importances.set_index(df_importances.columns[0], inplace=True)\n",
    "df_importances = df_importances[ranked_features]  # ensure correct column order\n",
    "\n",
    "# --- Compute mean importance ---\n",
    "mean_importances = df_importances.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original Rank': range(1, len(ranked_features)+1),\n",
    "    'Feature': ranked_features\n",
    "}).set_index('Feature')\n",
    "\n",
    "comparison_df['Mean Importance'] = mean_importances\n",
    "comparison_df = comparison_df.sort_values('Mean Importance', ascending=False)\n",
    "comparison_df['Importance Rank'] = range(1, len(comparison_df)+1)\n",
    "\n",
    "# --- Display results ---\n",
    "print(\"=== Feature Importance Comparison ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# --- Save comparison_df as image using matplotlib ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))  # height scales with number of rows\n",
    "ax.axis('off')\n",
    "\n",
    "# Render the table\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.values,\n",
    "    colLabels=comparison_df.columns,\n",
    "    rowLabels=comparison_df.index,\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)  # scale width and height\n",
    "\n",
    "# Save the table as image\n",
    "plt.savefig(table_image_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\n✅ Comparison table saved as image (matplotlib) to:\\n{table_image_path}\")\n",
    "\n",
    "# --- Plot and save ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Average Feature Importance Across Models\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"\\n✅ Plot saved to:\\n{plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3dfaac-95e9-46f2-af0d-21603b49cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Optional: for saving table image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Config: Manually specify CSV files to compare ---\n",
    "csv_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8_calc.csv'\n",
    "]\n",
    "\n",
    "# Feature names text file\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "\n",
    "# Output directory for plot and table\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "plot_path = os.path.join(output_dir, 'Overall_FI__calc_Comparison.png')\n",
    "table_path = os.path.join(output_dir, 'Overall_FI__calc_Table.png')\n",
    "\n",
    "# --- Load and clean feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    raw_features = [line.strip() for line in file if line.strip()]\n",
    "feature_list = [re.sub(r\"^\\d+\\.\\s*\", \"\", feat) for feat in raw_features]\n",
    "\n",
    "# --- Combine all manually listed CSVs ---\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    model_col = temp_df.columns[0]  # assume first column is model name\n",
    "    temp_df.set_index(model_col, inplace=True)\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    temp_df = temp_df[feature_list]\n",
    "    \n",
    "    temp_df['Dataset'] = os.path.basename(file)  # tag source\n",
    "    combined_df = pd.concat([combined_df, temp_df], axis=0)\n",
    "\n",
    "# --- Calculate average importance across all models and datasets ---\n",
    "mean_importances = combined_df[feature_list].mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# --- Create comparison DataFrame ---\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Mean Importance': mean_importances\n",
    "})\n",
    "comparison_df['Importance Rank'] = comparison_df['Mean Importance'].rank(ascending=False).astype(int)\n",
    "comparison_df = comparison_df.sort_values('Importance Rank')\n",
    "\n",
    "# --- Save table as image using matplotlib ---\n",
    "fig, ax = plt.subplots(figsize=(12, 0.5 + 0.4 * len(comparison_df)))\n",
    "ax.axis('off')\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.reset_index().values,\n",
    "    colLabels=['Feature'] + list(comparison_df.columns),\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.savefig(table_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"✅ Table saved to: {table_path}\")\n",
    "\n",
    "# --- Plot overall feature importance ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=mean_importances.index, y=mean_importances.values, palette='mako')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Overall Average Feature Importance Across All Datasets\")\n",
    "plt.ylabel(\"Mean Importance\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ Plot saved to: {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a345df0-a8d7-4f75-a0bf-aaf21688910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- Config: List of CSVs for both sets ---\n",
    "original_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8.csv'\n",
    "]\n",
    "\n",
    "# New set of CSVs to compare with\n",
    "new_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8_calc.csv'\n",
    "]\n",
    "\n",
    "# Feature names\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Output paths\n",
    "comparison_plot_path = os.path.join(output_dir, 'Feature_Importance_Comparison.png')\n",
    "comparison_table_path = os.path.join(output_dir, 'Feature_Importance_Comparison_Table.png')\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    raw_features = [line.strip() for line in file if line.strip()]\n",
    "feature_list = [re.sub(r\"^\\d+\\.\\s*\", \"\", feat) for feat in raw_features]\n",
    "\n",
    "# --- Helper function to load and average importance ---\n",
    "def load_and_average(files, label):\n",
    "    combined = pd.DataFrame()\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        model_col = df.columns[0]\n",
    "        df.set_index(model_col, inplace=True)\n",
    "        df = df[feature_list]  # keep only the correct features\n",
    "        combined = pd.concat([combined, df])\n",
    "    mean_importances = combined.mean(axis=0).sort_values(ascending=False)\n",
    "    return pd.DataFrame({label: mean_importances})\n",
    "\n",
    "# --- Load both sets ---\n",
    "original_importance = load_and_average(original_files, 'Original')\n",
    "new_importance = load_and_average(new_files, 'New')\n",
    "\n",
    "# --- Combine for comparison ---\n",
    "comparison_df = pd.concat([original_importance, new_importance], axis=1)\n",
    "comparison_df['Original Rank'] = comparison_df['Original'].rank(ascending=False).astype(int)\n",
    "comparison_df['New Rank'] = comparison_df['New'].rank(ascending=False).astype(int)\n",
    "comparison_df['Rank Change'] = comparison_df['Original Rank'] - comparison_df['New Rank']\n",
    "comparison_df = comparison_df.sort_values('Original Rank')\n",
    "\n",
    "# --- Save table as image ---\n",
    "fig, ax = plt.subplots(figsize=(14, 0.5 + 0.4 * len(comparison_df)))\n",
    "ax.axis('off')\n",
    "table = ax.table(\n",
    "    cellText=comparison_df.reset_index().values,\n",
    "    colLabels=['Feature'] + list(comparison_df.columns),\n",
    "    loc='center',\n",
    "    cellLoc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.2)\n",
    "plt.savefig(comparison_table_path, bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(f\"✅ Comparison table saved to: {comparison_table_path}\")\n",
    "\n",
    "# --- Plot side-by-side bar chart ---\n",
    "plt.figure(figsize=(14, 7))\n",
    "comparison_df[['Original', 'New']].plot.bar(rot=45, figsize=(16, 7), width=0.8)\n",
    "plt.title('Feature Importance Comparison (Original vs. New)')\n",
    "plt.ylabel('Mean Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(comparison_plot_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"✅ Comparison plot saved to: {comparison_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c5310-51a4-4b27-8a6f-ff56cead95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- File Paths ---\n",
    "original_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8.csv'\n",
    "]\n",
    "\n",
    "new_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8_calc.csv'\n",
    "]\n",
    "\n",
    "dataset_names = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Comparison_Plots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    raw_features = [line.strip() for line in file if line.strip()]\n",
    "feature_list = [re.sub(r\"^\\d+\\.\\s*\", \"\", feat) for feat in raw_features]\n",
    "\n",
    "# --- Loop through dataset pairs ---\n",
    "for original_path, new_path, name in zip(original_files, new_files, dataset_names):\n",
    "    try:\n",
    "        # --- Load original ---\n",
    "        df_orig = pd.read_csv(original_path)\n",
    "        df_orig.set_index(df_orig.columns[0], inplace=True)\n",
    "        df_orig = df_orig[feature_list]\n",
    "        orig_mean = df_orig.mean(axis=0)\n",
    "\n",
    "        # --- Load new ---\n",
    "        df_new = pd.read_csv(new_path)\n",
    "        df_new.set_index(df_new.columns[0], inplace=True)\n",
    "        df_new = df_new[feature_list]\n",
    "        new_mean = df_new.mean(axis=0)\n",
    "\n",
    "        # --- Combine ---\n",
    "        comparison_df = pd.DataFrame({\n",
    "            'Original': orig_mean,\n",
    "            'New': new_mean\n",
    "        })\n",
    "\n",
    "        # --- Ranking ---\n",
    "        comparison_df['Original Rank'] = comparison_df['Original'].rank(ascending=False).astype(int)\n",
    "        comparison_df['New Rank'] = comparison_df['New'].rank(ascending=False).astype(int)\n",
    "        comparison_df['Rank Change'] = comparison_df['Original Rank'] - comparison_df['New Rank']\n",
    "        comparison_df = comparison_df.sort_values('Original Rank')\n",
    "\n",
    "        # --- Save table ---\n",
    "        table_path = os.path.join(output_dir, f'Feature_Importance_Table_{name}.png')\n",
    "        fig, ax = plt.subplots(figsize=(14, 0.5 + 0.4 * len(comparison_df)))\n",
    "        ax.axis('off')\n",
    "        table = ax.table(\n",
    "            cellText=comparison_df.reset_index().values,\n",
    "            colLabels=['Feature'] + list(comparison_df.columns),\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.scale(1.2, 1.2)\n",
    "        plt.savefig(table_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Bar plot ---\n",
    "        plot_path = os.path.join(output_dir, f'Feature_Importance_Comparison_{name}.png')\n",
    "        comparison_df[['Original', 'New']].plot.bar(\n",
    "            figsize=(16, 7),\n",
    "            width=0.8\n",
    "        )\n",
    "        plt.title(f'Feature Importance Comparison - {name}')\n",
    "        plt.ylabel('Mean Importance')\n",
    "        plt.xlabel('Feature')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"✅ Comparison for {name} saved: Table and Plot.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfd8ca-bb73-4df9-ab57-ce574775dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds featue importance seperated by models \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "\n",
    "# --- File Paths ---\n",
    "original_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8.csv'\n",
    "]\n",
    "\n",
    "new_files = [\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS5_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7_calc.csv',\n",
    "    r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8_calc.csv'\n",
    "]\n",
    "\n",
    "dataset_names = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "\n",
    "features_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs\\Output7L_Clean.txt'\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Comparison_Plots'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- Load feature list ---\n",
    "with open(features_txt_path, 'r') as file:\n",
    "    raw_features = [line.strip() for line in file if line.strip()]\n",
    "feature_list = [re.sub(r\"^\\d+\\.\\s*\", \"\", feat) for feat in raw_features]\n",
    "\n",
    "def load_feature_importance(files, datasets, version_label):\n",
    "    \"\"\"\n",
    "    Load CSVs into a single DataFrame with multi-index: [dataset, model, version].\n",
    "    \"\"\"\n",
    "    combined = []\n",
    "    for file_path, ds_name in zip(files, datasets):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.set_index(df.columns[0], inplace=True)  # index = model names\n",
    "        # Filter to only features we want\n",
    "        df = df[feature_list]\n",
    "        # Add columns for metadata\n",
    "        df['dataset'] = ds_name\n",
    "        df['version'] = version_label\n",
    "        df['model'] = df.index\n",
    "        combined.append(df.reset_index(drop=True))\n",
    "    combined_df = pd.concat(combined, ignore_index=True)\n",
    "    combined_df.set_index(['dataset', 'model', 'version'], inplace=True)\n",
    "    return combined_df\n",
    "\n",
    "# Load data\n",
    "df_orig = load_feature_importance(original_files, dataset_names, 'Original')\n",
    "df_new = load_feature_importance(new_files, dataset_names, 'New')\n",
    "\n",
    "# Combine original and new data\n",
    "df_all = pd.concat([df_orig, df_new])\n",
    "\n",
    "# --- Now you have df_all indexed by dataset, model, version, columns are features\n",
    "\n",
    "# Let's create a comparison table per model with mean feature importance per dataset and version\n",
    "models = df_all.index.get_level_values('model').unique()\n",
    "\n",
    "for model in models:\n",
    "    try:\n",
    "        # Select data for this model across datasets and versions\n",
    "        df_model = df_all.xs(model, level='model')\n",
    "        \n",
    "        # Average feature importance per dataset and version (just in case multiple rows per model/dataset/version)\n",
    "        df_model_mean = df_model.groupby(['dataset', 'version']).mean()\n",
    "        \n",
    "        # Pivot to have versions side-by-side for each dataset for easy comparison\n",
    "        comparison = df_model_mean.unstack(level='version')\n",
    "        \n",
    "        # Flatten multiindex columns for easier visualization\n",
    "        comparison.columns = ['{}_{}'.format(feat, ver) for feat, ver in comparison.columns]\n",
    "        \n",
    "        # Save comparison table as CSV\n",
    "        csv_path = os.path.join(output_dir, f'Feature_Importance_Comparison_{model}.csv')\n",
    "        comparison.to_csv(csv_path)\n",
    "        \n",
    "        # Plot heatmap of feature importance for this model across datasets and versions\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        sns.heatmap(comparison.fillna(0), annot=True, fmt=\".3f\", cmap=\"coolwarm\")\n",
    "        plt.title(f'Feature Importance Comparison for Model: {model}')\n",
    "        plt.ylabel('Dataset')\n",
    "        plt.xlabel('Feature_Version')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'Feature_Importance_Heatmap_{model}.png'), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"✅ Saved comparison table and heatmap for model: {model}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing model {model}: {e}\")\n",
    "\n",
    "print(\"🎉 All done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c8564-b567-4ab6-933b-0ad3832f2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kendall tau meausrements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    # Calculate number of adjacent swaps (Kendall tau distance)\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:  # inverted pair\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# Example: load one CSV\n",
    "df_orig = pd.read_csv('Feature_Importance_DS3.csv')\n",
    "df_new = pd.read_csv('Feature_Importance_DS3_calc.csv')\n",
    "\n",
    "# The first column is model names\n",
    "df_orig.set_index(df_orig.columns[0], inplace=True)\n",
    "df_new.set_index(df_new.columns[0], inplace=True)\n",
    "\n",
    "feature_list = list(df_orig.columns)\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    try:\n",
    "        orig_importance = df_orig.loc[model, feature_list]\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "        # Rank features (1 = highest importance)\n",
    "        orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "        new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "        distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "        results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing model {model}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b6fde-169f-4667-b1da-e3c9393e3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv'\n",
    "\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model (first column) averaging importance if duplicates exist\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    try:\n",
    "        orig_importance = df_orig.loc[model, feature_list]\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "        # Rank features (1 = most important)\n",
    "        orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "        new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "        distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "        results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing model {model}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57895402-f7e9-4e5c-8fe7-3be5e26c21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv'\n",
    "\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model (first column) averaging importance if duplicates exist\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    try:\n",
    "        orig_importance = df_orig.loc[model].reindex(feature_list).fillna(0)\n",
    "        new_importance = df_new.loc[model].reindex(feature_list).fillna(0)\n",
    "\n",
    "        # Rank features (1 = most important)\n",
    "        orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "        new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "        distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "        results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing model {model}: {e}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a6a39f-447f-42a7-8d59-d0bc7c9c26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv'\n",
    "\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping or assigning zeros\")\n",
    "        # Option 1: skip model\n",
    "        # continue\n",
    "        # Option 2: assign zeros to all features in new data for this model\n",
    "        new_importance = pd.Series(0, index=feature_list)\n",
    "    else:\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "    orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "    # Align indices and fill missing features with zero\n",
    "    orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "    new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "    # Rank features (1 = most important)\n",
    "    orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "    new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "    distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "    results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d1164-2c24-4b25-b5ee-edcef28fb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: your original and new datasets as DataFrames with 'model' as index\n",
    "# df_orig and df_new should be pre-loaded or created before this\n",
    "\n",
    "# Example input (replace with your actual data loading)\n",
    "# df_orig = pd.read_csv('original_feature_importances.csv', index_col='model')\n",
    "# df_new = pd.read_csv('new_feature_importances.csv', index_col='model')\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping comparison\")\n",
    "        continue  # Skip models missing in new data\n",
    "    \n",
    "    # Get feature importance arrays for both datasets (assuming feature columns)\n",
    "    orig_features = df_orig.loc[model].values\n",
    "    new_features = df_new.loc[model].values\n",
    "    \n",
    "    # Compute Kendall tau distance or tau coefficient\n",
    "    tau, _ = kendalltau(orig_features, new_features)\n",
    "    \n",
    "    # If you want distance as in #adjacent swaps, convert accordingly or just store tau\n",
    "    results.append({\n",
    "        'model': model,\n",
    "        'kendall_tau': tau\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Sort by tau or descending distance as needed\n",
    "df_results = df_results.sort_values(by='kendall_tau')\n",
    "\n",
    "df_results.to_csv('kendall_tau_comparison_results.csv', index=False)\n",
    "\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad95c6-7946-4630-be89-80b5e387f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "# Load your original and new datasets here\n",
    "# Example:\n",
    "# df_orig = pd.read_csv('original_feature_importances.csv', index_col='model')\n",
    "# df_new = pd.read_csv('new_feature_importances.csv', index_col='model')\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping comparison\")\n",
    "        continue\n",
    "    \n",
    "    orig_features = df_orig.loc[model].values\n",
    "    new_features = df_new.loc[model].values\n",
    "    \n",
    "    n = len(orig_features)  # number of features\n",
    "    \n",
    "    tau, _ = kendalltau(orig_features, new_features)\n",
    "    \n",
    "    # Calculate number of adjacent swaps required\n",
    "    max_swaps = n * (n - 1) / 2\n",
    "    swaps_required = int(max_swaps * (1 - tau) / 2)\n",
    "    \n",
    "    results.append({\n",
    "        'model': model,\n",
    "        'kendall_tau': tau,\n",
    "        'kendall_tau_adjacent_swaps': swaps_required\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save results to CSV\n",
    "df_results.to_csv('kendall_tau_comparison_with_swaps.csv', index=False)\n",
    "\n",
    "print(\"Comparison results with swaps saved to 'kendall_tau_comparison_with_swaps.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dba7a9-2cd1-4abe-a1f7-e3b81de55910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kendalltau\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping comparison\")\n",
    "        continue\n",
    "    \n",
    "    orig_features = df_orig.loc[model].values\n",
    "    new_features = df_new.loc[model].values\n",
    "    \n",
    "    n = len(orig_features)\n",
    "    \n",
    "    # Skip if less than 2 features (Kendalltau requires at least 2)\n",
    "    if n < 2:\n",
    "        print(f\"Model {model} has less than 2 features, skipping\")\n",
    "        continue\n",
    "    \n",
    "    tau, _ = kendalltau(orig_features, new_features)\n",
    "    \n",
    "    if np.isnan(tau):\n",
    "        print(f\"Model {model} produced NaN tau, assigning swaps to 0\")\n",
    "        swaps_required = 0\n",
    "    else:\n",
    "        max_swaps = n * (n - 1) / 2\n",
    "        swaps_required = int(max_swaps * (1 - tau) / 2)\n",
    "    \n",
    "    results.append({\n",
    "        'model': model,\n",
    "        'kendall_tau': tau,\n",
    "        'kendall_tau_adjacent_swaps': swaps_required\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('kendall_tau_comparison_with_swaps.csv', index=False)\n",
    "print(\"Comparison results with swaps saved to 'kendall_tau_comparison_with_swaps.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c6424-c147-4167-b610-145e31a55d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define Kendall Tau distance as number of discordant pairs\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# File paths\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv'\n",
    "\n",
    "# Load the data\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model column (assumes first column is model name) and average feature importances\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "# Get list of features\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "# Compute Kendall tau distances\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping or assigning zeros\")\n",
    "        new_importance = pd.Series(0, index=feature_list)\n",
    "    else:\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "    orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "    # Align indices\n",
    "    orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "    new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "    # Rank features (1 = most important)\n",
    "    orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "    new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "    distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "    results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Display\n",
    "print(results_df)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(results_df, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Kendall Tau Distance (Swaps)'})\n",
    "plt.title('Feature Ranking Differences by Model')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 🔽 Save the plot as a PNG file (you can change the filename and format)\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\kendall_tau_heatmapDS3.png'\n",
    "plt.savefig(output_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2065f3e-500e-4781-88aa-e9b16e7ac353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c048c9-ed14-4059-a9c2-77cc37b6d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define Kendall Tau distance as number of discordant pairs\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# File paths\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS4_calc.csv'\n",
    "\n",
    "# Load the data\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model column (assumes first column is model name) and average feature importances\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "# Get list of features\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "# Compute Kendall tau distances\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping or assigning zeros\")\n",
    "        new_importance = pd.Series(0, index=feature_list)\n",
    "    else:\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "    orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "    # Align indices\n",
    "    orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "    new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "    # Rank features (1 = most important)\n",
    "    orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "    new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "    distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "    results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Display\n",
    "print(results_df)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(results_df, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Kendall Tau Distance (Swaps)'})\n",
    "plt.title('Feature Ranking Differences by Model')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 🔽 Save the plot as a PNG file (you can change the filename and format)\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\kendall_tau_heatmapDS4.png'\n",
    "plt.savefig(output_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1bcd7-9269-4b47-a269-5d10dbbd4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define Kendall Tau distance as number of discordant pairs\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# File paths\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS3_calc.csv'\n",
    "\n",
    "# Load the data\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model column (assumes first column is model name) and average feature importances\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "# Get list of features\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "# Compute Kendall tau distances\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping or assigning zeros\")\n",
    "        new_importance = pd.Series(0, index=feature_list)\n",
    "    else:\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "    orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "    # Align indices\n",
    "    orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "    new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "    # Rank features (1 = most important)\n",
    "    orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "    new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "    distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "    results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Display\n",
    "print(results_df)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(results_df, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Kendall Tau Distance (Swaps)'})\n",
    "plt.title('Feature Ranking Differences by Model')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 🔽 Save the plot as a PNG file (you can change the filename and format)\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\kendall_tau_heatmapDS5.png'\n",
    "plt.savefig(output_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4ae8cf-8f4e-45c1-8c8a-d043f8e96a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define Kendall Tau distance as number of discordant pairs\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# File paths\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS6_calc.csv'\n",
    "\n",
    "# Load the data\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model column (assumes first column is model name) and average feature importances\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "# Get list of features\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "# Compute Kendall tau distances\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping or assigning zeros\")\n",
    "        new_importance = pd.Series(0, index=feature_list)\n",
    "    else:\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "    orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "    # Align indices\n",
    "    orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "    new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "    # Rank features (1 = most important)\n",
    "    orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "    new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "    distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "    results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Display\n",
    "print(results_df)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(results_df, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Kendall Tau Distance (Swaps)'})\n",
    "plt.title('Feature Ranking Differences by Model')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 🔽 Save the plot as a PNG file (you can change the filename and format)\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\kendall_tau_heatmapDS6.png'\n",
    "plt.savefig(output_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1a182-ef7c-4397-9380-bcac97af3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define Kendall Tau distance as number of discordant pairs\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# File paths\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7_calc.csv'\n",
    "\n",
    "# Load the data\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model column (assumes first column is model name) and average feature importances\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "# Get list of features\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "# Compute Kendall tau distances\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping or assigning zeros\")\n",
    "        new_importance = pd.Series(0, index=feature_list)\n",
    "    else:\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "    orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "    # Align indices\n",
    "    orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "    new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "    # Rank features (1 = most important)\n",
    "    orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "    new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "    distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "    results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Display\n",
    "print(results_df)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(results_df, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Kendall Tau Distance (Swaps)'})\n",
    "plt.title('Feature Ranking Differences by Model')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 🔽 Save the plot as a PNG file (you can change the filename and format)\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\kendall_tau_heatmapDS7.png'\n",
    "plt.savefig(output_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1243c07-0a09-424e-975f-3faed7943db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define Kendall Tau distance as number of discordant pairs\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# File paths\n",
    "original_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8.csv'\n",
    "new_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS8_calc.csv'\n",
    "\n",
    "# Load the data\n",
    "df_orig = pd.read_csv(original_path)\n",
    "df_new = pd.read_csv(new_path)\n",
    "\n",
    "# Group by model column (assumes first column is model name) and average feature importances\n",
    "df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "# Get list of features\n",
    "feature_list = df_orig.columns.tolist()\n",
    "\n",
    "# Compute Kendall tau distances\n",
    "results = []\n",
    "\n",
    "for model in df_orig.index:\n",
    "    if model not in df_new.index:\n",
    "        print(f\"Model {model} missing in new data, skipping or assigning zeros\")\n",
    "        new_importance = pd.Series(0, index=feature_list)\n",
    "    else:\n",
    "        new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "    orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "    # Align indices\n",
    "    orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "    new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "    # Rank features (1 = most important)\n",
    "    orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "    new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "    distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "    results.append({'model': model, 'kendall_tau_adjacent_swaps': distance})\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('model', inplace=True)\n",
    "\n",
    "# Display\n",
    "print(results_df)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(results_df, annot=True, cmap='YlOrRd', cbar_kws={'label': 'Kendall Tau Distance (Swaps)'})\n",
    "plt.title('Feature Ranking Differences by Model')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# 🔽 Save the plot as a PNG file (you can change the filename and format)\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\kendall_tau_heatmapDS8.png'\n",
    "plt.savefig(output_path, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310dcead-ef38-48b4-8911-755b618aeb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_results = []\n",
    "\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "\n",
    "for ds in datasets:\n",
    "    original_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    new_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    # Load data\n",
    "    df_orig = pd.read_csv(original_path)\n",
    "    df_new = pd.read_csv(new_path)\n",
    "\n",
    "    df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "    df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "    feature_list = df_orig.columns.tolist()\n",
    "\n",
    "    # Reuse kendall_tau_distance from your code\n",
    "    def kendall_tau_distance(rank1, rank2):\n",
    "        swaps = 0\n",
    "        n = len(rank1)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                a = rank1[i] - rank1[j]\n",
    "                b = rank2[i] - rank2[j]\n",
    "                if a * b < 0:\n",
    "                    swaps += 1\n",
    "        return swaps\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in df_orig.index:\n",
    "        if model not in df_new.index:\n",
    "            new_importance = pd.Series(0, index=feature_list)\n",
    "        else:\n",
    "            new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "        orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "        orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "        new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "        orig_rank = orig_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "        new_rank = new_importance.rank(ascending=False, method='min').astype(int).tolist()\n",
    "\n",
    "        distance = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "        results.append({'model': model, 'kendall_tau_adjacent_swaps': distance, 'dataset': ds})\n",
    "\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Combine all results into one DataFrame\n",
    "df_all = pd.DataFrame(all_results)\n",
    "\n",
    "# Pivot to wide format: models as rows, datasets as columns\n",
    "heatmap_data = df_all.pivot(index='model', columns='dataset', values='kendall_tau_adjacent_swaps')\n",
    "\n",
    "# Plot combined heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Kendall Tau Distance'})\n",
    "plt.title('Feature Ranking Differences Across Datasets and Models')\n",
    "plt.ylabel('Model')\n",
    "plt.xlabel('Dataset')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save heatmap if you want\n",
    "plt.savefig(r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bb8b69-ed4a-4dd0-ad25-9889f3bdb85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed combo prompt and also parsing, again... \n",
    "\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "Heatmap = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis.txt'\n",
    "output_csv_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Analysis_Llama3.2.csv'\n",
    "\n",
    "        full_prompt = f\"{prompt_part1}\\ {Heatmap}\"\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                'http://localhost:11434/api/generate',\n",
    "                json={\n",
    "                    'model': 'llama3.2',\n",
    "                    'prompt': full_prompt,\n",
    "                    'stream': False,\n",
    "                    'temperature': 0.0,\n",
    "                    'top_p': 1.0\n",
    "                }\n",
    "            )\n",
    "            raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "print(\"✅ Ollama prompt generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21c763-28e5-43b6-831c-088228f0dcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Llama3.2.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'llama3.2',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05110f72-ca53-4227-a3b5-9457a6d007d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_gemma3.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'gemma3',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec75e5-508e-4e03-8c14-7fd74b0c4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_mistral.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'mistral',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cea7e4-2d61-48ca-a31b-45657c392985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_deepseek-r1.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'deepseek-r1',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accea061-46ad-43c7-b5dc-b24d3e51fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cayley distance emasures\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_results = []\n",
    "\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "\n",
    "# Cayley distance function\n",
    "def cayley_distance(rank1, rank2):\n",
    "    n = len(rank1)\n",
    "    \n",
    "    # Create a permutation that maps rank1 to rank2\n",
    "    perm = [0] * n\n",
    "    for i in range(n):\n",
    "        # Find position of i in rank1\n",
    "        pos = rank1.index(i)\n",
    "        perm[i] = rank2[pos]\n",
    "    \n",
    "    # Count number of cycles in the permutation\n",
    "    visited = [False] * n\n",
    "    cycles = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            j = i\n",
    "            while not visited[j]:\n",
    "                visited[j] = True\n",
    "                j = perm[j]\n",
    "            cycles += 1\n",
    "    \n",
    "    return n - cycles\n",
    "\n",
    "for ds in datasets:\n",
    "    original_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    new_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    # Load data\n",
    "    df_orig = pd.read_csv(original_path)\n",
    "    df_new = pd.read_csv(new_path)\n",
    "\n",
    "    df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "    df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "    feature_list = df_orig.columns.tolist()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in df_orig.index:\n",
    "        if model not in df_new.index:\n",
    "            new_importance = pd.Series(0, index=feature_list)\n",
    "        else:\n",
    "            new_importance = df_new.loc[model, feature_list]\n",
    "\n",
    "        orig_importance = df_orig.loc[model, feature_list]\n",
    "\n",
    "        orig_importance = orig_importance.reindex(feature_list).fillna(0)\n",
    "        new_importance = new_importance.reindex(feature_list).fillna(0)\n",
    "\n",
    "        orig_rank = (-orig_importance).argsort().argsort().tolist()\n",
    "        new_rank = (-new_importance).argsort().argsort().tolist()\n",
    "\n",
    "        distance = cayley_distance(orig_rank, new_rank)\n",
    "\n",
    "        results.append({'model': model, 'cayley_distance': distance, 'dataset': ds})\n",
    "\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Combine all results into one DataFrame\n",
    "df_all = pd.DataFrame(all_results)\n",
    "\n",
    "# Pivot to wide format: models as rows, datasets as columns\n",
    "heatmap_data = df_all.pivot(index='model', columns='dataset', values='cayley_distance')\n",
    "\n",
    "# Plot combined heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Cayley Distance'})\n",
    "plt.title('Feature Ranking Differences Across Datasets and Models (Cayley Distance)')\n",
    "plt.ylabel('Model')\n",
    "plt.xlabel('Dataset')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save heatmap\n",
    "plt.savefig(r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34603b0f-9329-4579-aaf6-85442439380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap cayley\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_deepseek-r1.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'deepseek-r1',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef4a2c-0024-42a9-bda1-130eb41e999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap cayley\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_llama3.2.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'Llama3.2',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2d1d7-ff62-4c1e-941b-df1dd47fe35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap cayley\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_mistral.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'mistral',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4a828-5311-45e8-baed-aee9006129a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap cayley\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_gemma3.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'gemma3',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f0e15-f70e-4edf-a706-f4ce3cc70e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "cayley_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "kendall_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley_vs_Kendall.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_vs_Kendall_gemma3.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = (\n",
    "    f\"{prompt_template}\\n\\n\"\n",
    "    f\"[Cayley Distance Heatmap Image Path: {cayley_heatmap_path}]\\n\"\n",
    "    f\"[Kendall Tau Distance Heatmap Image Path: {kendall_heatmap_path}]\"\n",
    ")\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'gemma3',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27a1a2-1de9-4a5b-8ad5-b3b1719a1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "cayley_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "kendall_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley_vs_Kendall.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_vs_Kendall_mistral.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = (\n",
    "    f\"{prompt_template}\\n\\n\"\n",
    "    f\"[Cayley Distance Heatmap Image Path: {cayley_heatmap_path}]\\n\"\n",
    "    f\"[Kendall Tau Distance Heatmap Image Path: {kendall_heatmap_path}]\"\n",
    ")\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'mistral',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2163acf-aca9-492c-9b3d-a019e413b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "cayley_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "kendall_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley_vs_Kendall.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_vs_Kendall_deepseek-r1.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = (\n",
    "    f\"{prompt_template}\\n\\n\"\n",
    "    f\"[Cayley Distance Heatmap Image Path: {cayley_heatmap_path}]\\n\"\n",
    "    f\"[Kendall Tau Distance Heatmap Image Path: {kendall_heatmap_path}]\"\n",
    ")\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'deepseek-r1',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd88d0-8a3d-4279-adfc-a67eca6baa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "cayley_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "kendall_heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png'\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley_vs_Kendall.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_vs_Kendall_Llama3.2.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = (\n",
    "    f\"{prompt_template}\\n\\n\"\n",
    "    f\"[Cayley Distance Heatmap Image Path: {cayley_heatmap_path}]\\n\"\n",
    "    f\"[Kendall Tau Distance Heatmap Image Path: {kendall_heatmap_path}]\"\n",
    ")\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'Llama3.2',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12ea2d-9024-4ed4-ab74-fef978058692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import rel_entr  # KL divergence\n",
    "\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "all_results_kl = []\n",
    "\n",
    "for ds in datasets:\n",
    "    original_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    new_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    df_orig = pd.read_csv(original_path)\n",
    "    df_new = pd.read_csv(new_path)\n",
    "\n",
    "    df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "    df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "    feature_list = df_orig.columns.tolist()\n",
    "\n",
    "    results = []\n",
    "    epsilon = 1e-12  # smoothing to avoid log(0)\n",
    "\n",
    "    for model in df_orig.index:\n",
    "        orig_importance = df_orig.loc[model, feature_list].reindex(feature_list).fillna(0)\n",
    "        new_importance = df_new.loc[model, feature_list].reindex(feature_list).fillna(0) if model in df_new.index else pd.Series(0, index=feature_list)\n",
    "\n",
    "        # Normalize to probability distributions\n",
    "        p = (orig_importance + epsilon) / (orig_importance + epsilon).sum()\n",
    "        q = (new_importance + epsilon) / (new_importance + epsilon).sum()\n",
    "\n",
    "        kl_divergence = np.sum(rel_entr(p, q))  # KL(P || Q)\n",
    "\n",
    "        results.append({'model': model, 'kl_divergence': kl_divergence, 'dataset': ds})\n",
    "\n",
    "    all_results_kl.extend(results)\n",
    "\n",
    "df_kl = pd.DataFrame(all_results_kl)\n",
    "heatmap_kl = df_kl.pivot(index='model', columns='dataset', values='kl_divergence')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_kl, annot=True, cmap='YlOrRd', cbar_kws={'label': 'KL Divergence'})\n",
    "plt.title('KL Divergence of Feature Importances (Original || New)')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6367ba9-bdeb-458a-a3d9-d8ed36c7ce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "all_results_emd = []\n",
    "\n",
    "for ds in datasets:\n",
    "    original_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    new_path = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    df_orig = pd.read_csv(original_path)\n",
    "    df_new = pd.read_csv(new_path)\n",
    "\n",
    "    df_orig = df_orig.groupby(df_orig.columns[0]).mean()\n",
    "    df_new = df_new.groupby(df_new.columns[0]).mean()\n",
    "\n",
    "    feature_list = df_orig.columns.tolist()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for model in df_orig.index:\n",
    "        orig_importance = df_orig.loc[model, feature_list].reindex(feature_list).fillna(0)\n",
    "        new_importance = df_new.loc[model, feature_list].reindex(feature_list).fillna(0) if model in df_new.index else pd.Series(0, index=feature_list)\n",
    "\n",
    "        # Normalize to ensure comparable scale (optional)\n",
    "        orig_norm = orig_importance / orig_importance.sum()\n",
    "        new_norm = new_importance / new_importance.sum()\n",
    "\n",
    "        emd = wasserstein_distance(orig_norm, new_norm)\n",
    "\n",
    "        results.append({'model': model, 'emd': emd, 'dataset': ds})\n",
    "\n",
    "    all_results_emd.extend(results)\n",
    "\n",
    "df_emd = pd.DataFrame(all_results_emd)\n",
    "heatmap_emd = df_emd.pivot(index='model', columns='dataset', values='emd')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heatmap_emd, annot=True, cmap='YlGnBu', cbar_kws={'label': 'Earth Mover\\'s Distance'})\n",
    "plt.title('EMD of Feature Importances Across Datasets and Models')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_emd_heatmap.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73a56f-95f7-442e-abb0-ff8d26e13d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap KL\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_KT.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_KT_gemma3.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'gemma3',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e2c619-4c31-41be-8c9c-e5619e7ba832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap KL\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_KT.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_KT_mistral.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'mistral',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c90fb8-b97b-4cce-9229-df2cec711953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap KL\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_KT.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_KT_deepseek-r1.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'deepseek-r1',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b760291-7b01-4396-8653-d082fee5b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap KL\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_KT.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_KT_Llama3.2.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'Llama3.2',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da0827-19f2-40c8-9760-44896371b63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap cayley\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_Cayley.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_Cayley_gemma3.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'gemma3',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fb464-3b72-4d40-9611-1c770957c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap EMD\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_EMD_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_EMD.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_EMD_gemma3.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'gemma3',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79882b-22a8-4c38-a0d2-91910443b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap EMD\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_EMD_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_EMD.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_EMD_mistral.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'mistral',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e926fbc-3451-44ca-922a-3d01ab74bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap EMD\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_EMD_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_EMD.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_EMD_deepseek-r1.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'deepseek-r1',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f834d-8bcc-4087-aa48-d47d7a87a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asking LLMs for insights on heatmap EMD\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# === File Paths ===\n",
    "heatmap_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_EMD_divergence_heatmap.png'\n",
    "prompt_part1_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_EMD.txt'\n",
    "output_txt_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_EMD_Llama3.2.txt'\n",
    "\n",
    "# === Read the prompt template ===\n",
    "with open(prompt_part1_path, 'r') as file:\n",
    "    prompt_part1 = file.read()\n",
    "\n",
    "# === Construct the full prompt ===\n",
    "full_prompt = f\"{prompt_part1}\\n\\n[Heatmap Image Path: {heatmap_path}]\"\n",
    "\n",
    "# === Call the LLM ===\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'Llama3.2',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    # === Print and save the response ===\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_txt_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ff8e4-893a-42cf-ae82-bbb963e7f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# === Datasets ===\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "distance_results = {'KL': [], 'EMD': [], 'Kendall': [], 'Cayley': []}\n",
    "\n",
    "# === Distance Functions ===\n",
    "\n",
    "def normalize(prob_dist):\n",
    "    total = np.sum(prob_dist)\n",
    "    return prob_dist / total if total > 0 else np.ones_like(prob_dist) / len(prob_dist)\n",
    "\n",
    "def cayley_distance(rank1, rank2):\n",
    "    n = len(rank1)\n",
    "    perm = [rank2[rank1.index(i)] for i in range(n)]\n",
    "    visited = [False] * n\n",
    "    cycles = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            j = i\n",
    "            while not visited[j]:\n",
    "                visited[j] = True\n",
    "                j = perm[j]\n",
    "            cycles += 1\n",
    "    return n - cycles\n",
    "\n",
    "def kendall_tau_distance(rank1, rank2):\n",
    "    swaps = 0\n",
    "    n = len(rank1)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            a = rank1[i] - rank1[j]\n",
    "            b = rank2[i] - rank2[j]\n",
    "            if a * b < 0:\n",
    "                swaps += 1\n",
    "    return swaps\n",
    "\n",
    "# === Loop through datasets ===\n",
    "\n",
    "for ds in datasets:\n",
    "    path_orig = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    path_new = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    # Read data safely\n",
    "    df_orig_raw = pd.read_csv(path_orig)\n",
    "    df_new_raw = pd.read_csv(path_new)\n",
    "\n",
    "    group_col = df_orig_raw.columns[0]\n",
    "\n",
    "    df_orig = df_orig_raw.groupby(group_col).mean(numeric_only=True)\n",
    "    df_new = df_new_raw.groupby(group_col).mean(numeric_only=True)\n",
    "\n",
    "    features = df_orig.columns.tolist()\n",
    "\n",
    "    for model in df_orig.index:\n",
    "        if model not in df_new.index:\n",
    "            new_importance = pd.Series(0, index=features)\n",
    "        else:\n",
    "            new_importance = df_new.loc[model, features]\n",
    "\n",
    "        orig_importance = df_orig.loc[model, features]\n",
    "\n",
    "        orig_importance = orig_importance.reindex(features).fillna(0)\n",
    "        new_importance = new_importance.reindex(features).fillna(0)\n",
    "\n",
    "        # Normalize for probability-based metrics\n",
    "        orig_prob = normalize(orig_importance.values)\n",
    "        new_prob = normalize(new_importance.values)\n",
    "\n",
    "        # Rankings (0 = best)\n",
    "        orig_rank = (-orig_importance).argsort().argsort().tolist()\n",
    "        new_rank = (-new_importance).argsort().argsort().tolist()\n",
    "\n",
    "        # Distances\n",
    "        kl_div = entropy(orig_prob, new_prob)\n",
    "        emd_dist = wasserstein_distance(orig_prob, new_prob)\n",
    "        cayley = cayley_distance(orig_rank, new_rank)\n",
    "        kendall = kendall_tau_distance(orig_rank, new_rank)\n",
    "\n",
    "        distance_results['KL'].append({'model': model, 'dataset': ds, 'value': kl_div})\n",
    "        distance_results['EMD'].append({'model': model, 'dataset': ds, 'value': emd_dist})\n",
    "        distance_results['Cayley'].append({'model': model, 'dataset': ds, 'value': cayley})\n",
    "        distance_results['Kendall'].append({'model': model, 'dataset': ds, 'value': kendall})\n",
    "\n",
    "# === Generate Heatmaps ===\n",
    "\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "for dist_name, results in distance_results.items():\n",
    "    df = pd.DataFrame(results)\n",
    "    pivoted = df.pivot(index='model', columns='dataset', values='value')\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pivoted, annot=True, cmap='viridis', cbar_kws={'label': f'{dist_name} Distance'})\n",
    "    plt.title(f'Feature Ranking Differences ({dist_name} Distance)')\n",
    "    plt.ylabel('Model')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save heatmap\n",
    "    heatmap_path = f'{output_dir}/combined_{dist_name.lower()}_heatmap.png'\n",
    "    plt.savefig(heatmap_path, dpi=300)\n",
    "    print(f\"✅ Saved heatmap: {heatmap_path}\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca5c94-00e6-4cae-a5c1-6b9de39a5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_All_Distances.txt'\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_All_Distances_Gemma.txt'\n",
    "\n",
    "heatmap_paths = {\n",
    "    'kendall': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png',\n",
    "    'cayley': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png',\n",
    "    'kl': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png',\n",
    "    'emd': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_emd_heatmap.png'\n",
    "}\n",
    "\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# Construct the full prompt\n",
    "heatmap_refs = '\\n'.join([f\"[{metric.upper()} Heatmap]: {path}\" for metric, path in heatmap_paths.items()])\n",
    "full_prompt = f\"{prompt_template}\\n\\n{heatmap_refs}\"\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'gemma3',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200dbc26-f947-405e-97f8-1c34592ebbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_All_Distances.txt'\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_All_Distances_Llama3.2.txt'\n",
    "\n",
    "heatmap_paths = {\n",
    "    'kendall': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png',\n",
    "    'cayley': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png',\n",
    "    'kl': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png',\n",
    "    'emd': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_emd_heatmap.png'\n",
    "}\n",
    "\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# Construct the full prompt\n",
    "heatmap_refs = '\\n'.join([f\"[{metric.upper()} Heatmap]: {path}\" for metric, path in heatmap_paths.items()])\n",
    "full_prompt = f\"{prompt_template}\\n\\n{heatmap_refs}\"\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'Llama3.2',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3593bf-7ec5-47fb-92b6-ae1cf8ad3688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_All_Distances.txt'\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_All_Distances_deepseek-r1.txt'\n",
    "\n",
    "heatmap_paths = {\n",
    "    'kendall': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png',\n",
    "    'cayley': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png',\n",
    "    'kl': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png',\n",
    "    'emd': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_emd_heatmap.png'\n",
    "}\n",
    "\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# Construct the full prompt\n",
    "heatmap_refs = '\\n'.join([f\"[{metric.upper()} Heatmap]: {path}\" for metric, path in heatmap_paths.items()])\n",
    "full_prompt = f\"{prompt_template}\\n\\n{heatmap_refs}\"\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'deepseek-r1',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a2e67-3219-44c9-921d-aa6f8f6488e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "prompt_path = r'C:\\PhD\\Prompt_Engineering\\Final_Prompts\\Heatmap_Analysis_All_Distances.txt'\n",
    "output_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Heatmap_Insights_All_Distances_mistral.txt'\n",
    "\n",
    "heatmap_paths = {\n",
    "    'kendall': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kendall_tau_heatmap.png',\n",
    "    'cayley': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_cayley_heatmap.png',\n",
    "    'kl': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_kl_divergence_heatmap.png',\n",
    "    'emd': r'C:\\PhD\\Prompt_Engineering\\Outputs2\\combined_emd_heatmap.png'\n",
    "}\n",
    "\n",
    "with open(prompt_path, 'r', encoding='utf-8') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "# Construct the full prompt\n",
    "heatmap_refs = '\\n'.join([f\"[{metric.upper()} Heatmap]: {path}\" for metric, path in heatmap_paths.items()])\n",
    "full_prompt = f\"{prompt_template}\\n\\n{heatmap_refs}\"\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        'http://localhost:11434/api/generate',\n",
    "        json={\n",
    "            'model': 'mistral',\n",
    "            'prompt': full_prompt,\n",
    "            'stream': False,\n",
    "            'temperature': 0.0,\n",
    "            'top_p': 1.0\n",
    "        }\n",
    "    )\n",
    "    raw_output = response.json().get('response', '').strip()\n",
    "\n",
    "    print(\"🔍 LLM Response:\\n\")\n",
    "    print(raw_output)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "        out_file.write(raw_output)\n",
    "\n",
    "    print(f\"\\n✅ Insights saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during LLM call: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab8a4c-83d3-4a21-9906-6118a504c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comapring by raw importance rather than rank \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "# === SETTINGS ===\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "distance_results = {'Cosine': [], 'Euclidean': []}\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "# === UTILITY ===\n",
    "\n",
    "def safe_importances(df, model, features):\n",
    "    \"\"\"Return importance vector or zeros if model is missing.\"\"\"\n",
    "    if model not in df.index:\n",
    "        return np.zeros(len(features))\n",
    "    return df.loc[model, features].fillna(0).values\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "\n",
    "for ds in datasets:\n",
    "    path_orig = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    path_new = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    df_orig_raw = pd.read_csv(path_orig)\n",
    "    df_new_raw = pd.read_csv(path_new)\n",
    "\n",
    "    group_col = df_orig_raw.columns[0]\n",
    "\n",
    "    df_orig = df_orig_raw.groupby(group_col).mean(numeric_only=True)\n",
    "    df_new = df_new_raw.groupby(group_col).mean(numeric_only=True)\n",
    "\n",
    "    features = df_orig.columns.tolist()\n",
    "\n",
    "    # Loop models\n",
    "    for model in df_orig.index:\n",
    "        \n",
    "        orig = safe_importances(df_orig, model, features)\n",
    "        new = safe_importances(df_new, model, features)\n",
    "\n",
    "        # --- METRICS ---\n",
    "\n",
    "        # Cosine similarity (handle zero vectors)\n",
    "        if np.all(orig == 0) or np.all(new == 0):\n",
    "            cos_sim = 0  # define as zero similarity for empty vectors\n",
    "        else:\n",
    "            cos_sim = 1 - cosine(orig, new)  # similarity (1 - cosine distance)\n",
    "\n",
    "        # Euclidean distance\n",
    "        euc_dist = euclidean(orig, new)\n",
    "\n",
    "        # Store\n",
    "        distance_results['Cosine'].append({\n",
    "            'model': model,\n",
    "            'dataset': ds,\n",
    "            'value': cos_sim\n",
    "        })\n",
    "        distance_results['Euclidean'].append({\n",
    "            'model': model,\n",
    "            'dataset': ds,\n",
    "            'value': euc_dist\n",
    "        })\n",
    "\n",
    "\n",
    "# === GENERATE HEATMAPS ===\n",
    "\n",
    "for dist_name, results in distance_results.items():\n",
    "    df = pd.DataFrame(results)\n",
    "    pivoted = df.pivot(index='model', columns='dataset', values='value')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        pivoted, \n",
    "        annot=True, \n",
    "        cmap='viridis',\n",
    "        cbar_kws={'label': f'{dist_name} Metric'}\n",
    "    )\n",
    "    plt.title(f'Feature Importance Changes LLM ({dist_name})')\n",
    "    plt.ylabel('Model')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    heatmap_path = f'{output_dir}/combined_LLM{dist_name.lower()}_heatmap.png'\n",
    "    plt.savefig(heatmap_path, dpi=300)\n",
    "    print(f\"Saved: {heatmap_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7dc91e-e73e-4894-981d-290aad1cddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comapring by raw importance rather than rank \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "# === SETTINGS ===\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "distance_results = {'Cosine': [], 'Euclidean': []}\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "# === UTILITY ===\n",
    "\n",
    "def safe_importances(df, model, features):\n",
    "    \"\"\"Return importance vector or zeros if model is missing.\"\"\"\n",
    "    if model not in df.index:\n",
    "        return np.zeros(len(features))\n",
    "    return df.loc[model, features].fillna(0).values\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "\n",
    "for ds in datasets:\n",
    "    path_orig = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    path_new = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    df_orig_raw = pd.read_csv(path_orig)\n",
    "    df_new_raw = pd.read_csv(path_new)\n",
    "\n",
    "    group_col = df_orig_raw.columns[0]\n",
    "\n",
    "    df_orig = df_orig_raw.groupby(group_col).mean(numeric_only=True)\n",
    "    df_new = df_new_raw.groupby(group_col).mean(numeric_only=True)\n",
    "\n",
    "    features = df_orig.columns.tolist()\n",
    "\n",
    "    # Loop models\n",
    "    for model in df_orig.index:\n",
    "        \n",
    "        orig = safe_importances(df_orig, model, features)\n",
    "        new = safe_importances(df_new, model, features)\n",
    "\n",
    "        # --- METRICS ---\n",
    "\n",
    "        # Cosine similarity (handle zero vectors)\n",
    "        if np.all(orig == 0) or np.all(new == 0):\n",
    "            cos_sim = 0  # define as zero similarity for empty vectors\n",
    "        else:\n",
    "            cos_sim = 1 - cosine(orig, new)  # similarity (1 - cosine distance)\n",
    "\n",
    "        # Euclidean distance\n",
    "        euc_dist = euclidean(orig, new)\n",
    "\n",
    "        # Store\n",
    "        distance_results['Cosine'].append({\n",
    "            'model': model,\n",
    "            'dataset': ds,\n",
    "            'value': cos_sim\n",
    "        })\n",
    "        distance_results['Euclidean'].append({\n",
    "            'model': model,\n",
    "            'dataset': ds,\n",
    "            'value': euc_dist\n",
    "        })\n",
    "\n",
    "\n",
    "# === GENERATE HEATMAPS ===\n",
    "\n",
    "for dist_name, results in distance_results.items():\n",
    "    df = pd.DataFrame(results)\n",
    "    pivoted = df.pivot(index='model', columns='dataset', values='value')\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(\n",
    "        pivoted, \n",
    "        annot=True, \n",
    "        cmap='viridis',\n",
    "        cbar_kws={'label': f'{dist_name} Metric'}\n",
    "    )\n",
    "    plt.title(f'Feature Importance Changes ({dist_name})')\n",
    "    plt.ylabel('Model')\n",
    "    plt.xlabel('Dataset')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    heatmap_path = f'{output_dir}/combined_{dist_name.lower()}_heatmap.png'\n",
    "    plt.savefig(heatmap_path, dpi=300)\n",
    "    print(f\"Saved: {heatmap_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c464f11-1818-48c1-b41a-5ee97ff2ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === SETTINGS ===\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "# === UTILITY ===\n",
    "def safe_importances(df, model, features):\n",
    "    \"\"\"Return importance vector or zeros if model missing.\"\"\"\n",
    "    if model not in df.index:\n",
    "        return np.zeros(len(features))\n",
    "    return df.loc[model, features].fillna(0).values\n",
    "\n",
    "\n",
    "# === RESULTS STORAGE ===\n",
    "all_feature_changes = []  # list of rows (dataset, model, feature, values)\n",
    "\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for ds in datasets:\n",
    "\n",
    "    print(f\"Processing {ds}...\")\n",
    "\n",
    "    path_orig = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    path_new  = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    df_orig_raw = pd.read_csv(path_orig)\n",
    "    df_new_raw  = pd.read_csv(path_new)\n",
    "\n",
    "    group_col = df_orig_raw.columns[0]\n",
    "\n",
    "    df_orig = df_orig_raw.groupby(group_col).mean(numeric_only=True)\n",
    "    df_new  = df_new_raw.groupby(group_col).mean(numeric_only=True)\n",
    "\n",
    "    features = df_orig.columns.tolist()\n",
    "\n",
    "    # Loop models\n",
    "    for model in df_orig.index:\n",
    "\n",
    "        orig = safe_importances(df_orig, model, features)\n",
    "        new = safe_importances(df_new, model, features)\n",
    "\n",
    "        # Compute per-feature changes\n",
    "        diff = new - orig\n",
    "        abs_diff = np.abs(diff)\n",
    "        pct_diff = np.where(orig != 0, diff / np.abs(orig), np.nan)\n",
    "\n",
    "        # Store long-form record\n",
    "        for i, feat in enumerate(features):\n",
    "            all_feature_changes.append({\n",
    "                \"dataset\": ds,\n",
    "                \"model\": model,\n",
    "                \"feature\": feat,\n",
    "                \"orig\": orig[i],\n",
    "                \"new\": new[i],\n",
    "                \"diff\": diff[i],\n",
    "                \"abs_diff\": abs_diff[i],\n",
    "                \"pct_change\": pct_diff[i]\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert to dataframe\n",
    "feature_df = pd.DataFrame(all_feature_changes)\n",
    "print(\"Completed feature-level comparison.\")\n",
    "\n",
    "\n",
    "# === HEATMAPS PER DATASET ===Cosi\n",
    "for ds in datasets:\n",
    "\n",
    "    subset = feature_df[feature_df['dataset'] == ds]\n",
    "\n",
    "    # Pivot: rows = models, columns = features, values = change amount\n",
    "    pivot = subset.pivot(index=\"model\", columns=\"feature\", values=\"diff\")\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(pivot, annot=False, cmap=\"coolwarm\", center=0,\n",
    "                cbar_kws={'label': 'Change (new - original)'})\n",
    "\n",
    "    plt.title(f\"Per-Feature Importance Change\\nDataset: {ds}\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = f\"{output_dir}/feature_change_heatmap_{ds}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    print(f\"Saved heatmap for {ds}: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# === OPTIONAL: Combined heatmap across datasets ===\n",
    "pivot_all = feature_df.pivot_table(\n",
    "    index=\"model\",\n",
    "    columns=\"feature\",\n",
    "    values=\"diff\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(pivot_all, annot=False, cmap=\"coolwarm\", center=0,\n",
    "            cbar_kws={'label': 'Mean Change Across Datasets'})\n",
    "plt.title(\"Per-Feature Importance Change (Averaged Across Datasets)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = f\"{output_dir}/combined_feature_change_heatmap.png\"\n",
    "plt.savefig(save_path, dpi=300)\n",
    "print(f\"Saved combined heatmap: {save_path}\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e1ffc-67d5-447e-92a4-2c623e8292f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === SETTINGS ===\n",
    "datasets = ['DS3', 'DS4', 'DS5', 'DS6', 'DS7', 'DS8']\n",
    "output_dir = r'C:\\PhD\\Prompt_Engineering\\Outputs2'\n",
    "\n",
    "\n",
    "# === UTILITY ===\n",
    "def safe_importances(df, model, features):\n",
    "    \"\"\"Return importance vector or zeros if missing.\"\"\"\n",
    "    if model not in df.index:\n",
    "        return np.zeros(len(features))\n",
    "    return df.loc[model, features].fillna(0).values\n",
    "\n",
    "\n",
    "# === RESULTS STORAGE ===\n",
    "orig_records = []\n",
    "calc_records = []\n",
    "\n",
    "\n",
    "# === MAIN LOOP FOR ORIGINAL + CALC SEPARATE STORAGE ===\n",
    "for ds in datasets:\n",
    "\n",
    "    print(f\"Processing dataset {ds} for original & calc visualisation...\")\n",
    "\n",
    "    path_orig = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}.csv'\n",
    "    path_calc = fr'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_{ds}_calc.csv'\n",
    "\n",
    "    df_orig_raw = pd.read_csv(path_orig)\n",
    "    df_calc_raw = pd.read_csv(path_calc)\n",
    "\n",
    "    group_col = df_orig_raw.columns[0]\n",
    "\n",
    "    df_orig = df_orig_raw.groupby(group_col).mean(numeric_only=True)\n",
    "    df_calc = df_calc_raw.groupby(group_col).mean(numeric_only=True)\n",
    "\n",
    "    features = df_orig.columns.tolist()\n",
    "\n",
    "    # Loop models\n",
    "    all_models = sorted(set(df_orig.index).union(set(df_calc.index)))\n",
    "\n",
    "    for model in all_models:\n",
    "\n",
    "        orig = safe_importances(df_orig, model, features)\n",
    "        calc = safe_importances(df_calc, model, features)\n",
    "\n",
    "        for i, feat in enumerate(features):\n",
    "\n",
    "            orig_records.append({\n",
    "                \"dataset\": ds,\n",
    "                \"model\": model,\n",
    "                \"feature\": feat,\n",
    "                \"importance\": orig[i]\n",
    "            })\n",
    "\n",
    "            calc_records.append({\n",
    "                \"dataset\": ds,\n",
    "                \"model\": model,\n",
    "                \"feature\": feat,\n",
    "                \"importance\": calc[i]\n",
    "            })\n",
    "\n",
    "\n",
    "# Convert to DataFrames\n",
    "orig_df = pd.DataFrame(orig_records)\n",
    "calc_df = pd.DataFrame(calc_records)\n",
    "\n",
    "print(\"Finished aggregating original and calculated importance.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# === 1️⃣ HEATMAPS: ORIGINAL FEATURE IMPORTANCES PER DATASET ===\n",
    "# ============================================================\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    subset = orig_df[orig_df['dataset'] == ds]\n",
    "\n",
    "    pivot = subset.pivot(index=\"model\", columns=\"feature\", values=\"importance\")\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(pivot, cmap=\"viridis\", annot=False,\n",
    "                cbar_kws={'label': 'Original Feature Importance'})\n",
    "\n",
    "    plt.title(f\"Original Feature Importances\\nDataset: {ds}\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = f\"{output_dir}/original_importances_heatmap_{ds}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    print(f\"Saved original heatmap: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# === 2️⃣ HEATMAPS: CALCULATED FEATURE IMPORTANCES PER DATASET ===\n",
    "# ==============================================================\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    subset = calc_df[calc_df['dataset'] == ds]\n",
    "\n",
    "    pivot = subset.pivot(index=\"model\", columns=\"feature\", values=\"importance\")\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(pivot, cmap=\"viridis\", annot=False,\n",
    "                cbar_kws={'label': 'Calculated Feature Importance'})\n",
    "\n",
    "    plt.title(f\"Calculated Feature Importances\\nDataset: {ds}\")\n",
    "    plt.ylabel(\"Model\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    save_path = f\"{output_dir}/calculated_importances_heatmap_{ds}.png\"\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    print(f\"Saved calculated heatmap: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# === OPTIONAL COMBINED HEATMAPS (AVERAGED ACROSS ALL DATASETS) =======\n",
    "# =====================================================================\n",
    "\n",
    "# Combined original\n",
    "pivot_orig_all = orig_df.pivot_table(\n",
    "    index=\"model\", columns=\"feature\", values=\"importance\", aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(pivot_orig_all, cmap=\"viridis\", annot=False,\n",
    "            cbar_kws={'label': 'Mean Original Importance'})\n",
    "plt.title(\"Original Feature Importances (Mean Across All Datasets)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = f\"{output_dir}/original_importances_combined.png\"\n",
    "plt.savefig(save_path, dpi=300)\n",
    "print(f\"Saved combined original heatmap: {save_path}\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Combined calculated\n",
    "pivot_calc_all = calc_df.pivot_table(\n",
    "    index=\"model\", columns=\"feature\", values=\"importance\", aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(pivot_calc_all, cmap=\"viridis\", annot=False,\n",
    "            cbar_kws={'label': 'Mean Calculated Importance'})\n",
    "plt.title(\"Calculated Feature Importances (Mean Across All Datasets)\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = f\"{output_dir}/calculated_importances_combined.png\"\n",
    "plt.savefig(save_path, dpi=300)\n",
    "print(f\"Saved combined calculated heatmap: {save_path}\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98026a72-a453-4637-ba8b-143af6ed943f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
