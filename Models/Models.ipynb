{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060e12f1-3b5d-4bc9-b659-8b5e72d53dd7",
   "metadata": {},
   "source": [
    "# Running datasets through models. Inputs and output file names need changed in 2,3 & 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b0196-d331-49ea-bcb2-5e8880f9e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fractions import Fraction\n",
    "\n",
    "df = pd.read_csv('DS7_Calc_Tox_clean.csv')\n",
    "\n",
    "def frac_str_to_float(frac_str):\n",
    "    try:\n",
    "        # Handle if already float or int\n",
    "        if pd.isna(frac_str):\n",
    "            return frac_str  # keep NaN as is\n",
    "        if isinstance(frac_str, (float, int)):\n",
    "            return frac_str\n",
    "        return float(Fraction(frac_str))\n",
    "    except Exception as e:\n",
    "        print(f\"Conversion error for value '{frac_str}': {e}\")\n",
    "        return None  # or np.nan\n",
    "\n",
    "# Convert the fraction columns\n",
    "df['Toxicophore_Presence'] = df['Toxicophore_Presence'].apply(frac_str_to_float)\n",
    "df['Alerting_Structural_Motifs'] = df['Alerting_Structural_Motifs'].apply(frac_str_to_float)\n",
    "\n",
    "# Check if any NaNs remain after conversion\n",
    "print(df[['Toxicophore_Presence', 'Alerting_Structural_Motifs']].isna().sum())\n",
    "\n",
    "# Save the fixed DataFrame back to the same CSV (overwrite)\n",
    "df.to_csv('DS7_Calc_Tox_clean.csv', index=False)\n",
    "\n",
    "print(\"âœ… Conversion complete and saved to original CSV.\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load your original CSV\n",
    "df = pd.read_csv('DS7_Calc_Tox_clean.csv')\n",
    "\n",
    "# Filter out rows where Toxicity is 'Inconclusive' (keep only 'Active' or 'Inactive')\n",
    "df_cleaned = df[df['Toxicity'].isin(['Active', 'Inactive'])].copy()\n",
    "\n",
    "# Save the cleaned DataFrame back to CSV (overwrite or specify a new file)\n",
    "df_cleaned.to_csv('DS7_Calc_Tox_clean.csv', index=False)\n",
    "\n",
    "print(f\"Rows with 'Inconclusive' Toxicity removed. Cleaned data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab28e64d-fa07-46d5-af33-2ddc078594bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated file path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "file_path = 'DS7_Calc_Tox_clean.csv'\n",
    "df1 = pd.read_csv(file_path)\n",
    "\n",
    "# Prepare the features (E-state indices) and target (Activity_Outcomes)\n",
    "X = df1[['Molecular_Weight', 'LogP', 'H_Acceptors', 'H_Donors', 'TPSA', 'Rotable_Bonds', 'Rings', 'Aromatic_Rings', 'SP3_Carbon', 'Toxicophore_Presence', 'Alerting_Structural_Motifs', 'QED', 'Lipinskis_Rule', 'Molar_Refractivity', 'Atoms_With_High_Ionization_Potential']]  # Features \n",
    "\n",
    "# Map 'Active' to 1 and 'Inactive' to 0\n",
    "y = df1['Toxicity'].map({'Active': 1, 'Inactive': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a941cf2-f7d3-4fb2-a984-2d56db9e12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e78c97-c115-4232-891d-b352dd016f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df7d07-e080-4f22-b228-bf82a9112563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Step 1: Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Standard scale features (fit on training only)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Apply SMOTE to only training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Optional: Print shapes\n",
    "print(f\"Original X_train shape: {X_train.shape}\")\n",
    "print(f\"Resampled X_train shape: {X_train_resampled.shape}\")\n",
    "print(f\"X_test shape (unchanged): {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd4c01-dee6-40a2-9993-ce015f6a9606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update paths\n",
    "# Paths\n",
    "output_csv_path = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Precision_DS7_calc.csv'\n",
    "feature_importance_csv = r'C:\\PhD\\Prompt_Engineering\\Outputs2\\Feature_Importance_DS7_calc.csv'\n",
    "\n",
    "# Your specific feature list\n",
    "feature_names = [\n",
    "    'Molecular_Weight', 'LogP', 'H_Acceptors', 'H_Donors', 'TPSA',\n",
    "    'Rotable_Bonds', 'Rings', 'Aromatic_Rings', 'SP3_Carbon',\n",
    "    'Toxicophore_Presence', 'Alerting_Structural_Motifs', 'QED',\n",
    "    'Lipinskis_Rule', 'Molar_Refractivity', 'Atoms_With_High_Ionization_Potential'\n",
    "]\n",
    "\n",
    "y_test_input = y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37d802-baca-4628-ba03-aac3e267362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard SVM -\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "model_title = \"SVM_original\"  # Change this title for each run\n",
    "\n",
    "# Step 1: Train the SVM\n",
    "svm_model = SVC(kernel='rbf', random_state=42, class_weight='balanced', probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Predict\n",
    "y_pred = svm_model.predict(X_test)\n",
    "y_proba = svm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Step 3: Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "\n",
    "# Step 4: Cross-validated recall\n",
    "cv_recall_scores = cross_val_score(\n",
    "    svm_model, X_train, y_train, cv=5, scoring='recall'\n",
    ")\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# Step 5: Save results to CSV\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "# Check if file exists to write header only once\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# --- Step 5: Permutation feature importance\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    svm_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose: model title as row, features as columns\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833058e-4216-4549-bb37-fd12e90d350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled SVM -\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "model_title = \"SVM_scaled\"  # Change this title for each run\n",
    "\n",
    "# Step 1: Train the SVM\n",
    "svm_model = SVC(kernel='rbf', random_state=42, class_weight='balanced', probability=True)\n",
    "svm_model.fit(pd.DataFrame(X_train_scaled, columns=feature_names), y_train)\n",
    "\n",
    "\n",
    "# Step 2: Predict\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "y_pred = svm_model.predict(X_test_scaled_df)\n",
    "y_proba = svm_model.predict_proba(X_test_scaled_df)[:, 1]\n",
    "\n",
    "\n",
    "# Step 3: Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "# Step 4: Cross-validated recall\n",
    "cv_recall_scores = cross_val_score(\n",
    "    svm_model, X_train, y_train, cv=5, scoring='recall'\n",
    ")\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# Step 5: Save results to CSV\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "\n",
    "# Check if file exists to write header only once\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# --- Step 5: Permutation feature importance\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    svm_model, X_test_scaled_df, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose: model title as row, features as columns\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05a044-d25a-4b41-84db-67373f6a174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE SVM\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ðŸ” Update this for each run\n",
    "model_title = \"SVM_scaled_SMOTE\"  # Change name for each model you run\n",
    "model = SVC(kernel='rbf', random_state=42, class_weight='balanced', probability=True)\n",
    "\n",
    "# ðŸ§  Choose your input data (update manually depending on the variant)\n",
    "X_train_input = X_train_resampled   # e.g., X_train, X_train_scaled, X_train_resampled\n",
    "y_train_input = y_train_resampled   # y_train or y_train_resampled\n",
    "X_test_input  = X_test_scaled       # e.g., X_test or X_test_scaled\n",
    "y_test_input  = y_test              # usually y_test stays the same\n",
    "\n",
    "# --- Step 1: Train model\n",
    "model.fit(X_train_input, y_train_input)\n",
    "y_pred = model.predict(X_test_input)\n",
    "y_proba = model.predict_proba(X_test_input)[:, 1]\n",
    "\n",
    "# --- Step 2: Evaluation metrics\n",
    "accuracy = accuracy_score(y_test_input, y_pred)\n",
    "recall = recall_score(y_test_input, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_input, y_pred)\n",
    "auc = roc_auc_score(y_test_input, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_input, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "\n",
    "# --- Step 3: Cross-validated recall (no resampling, fair comparison)\n",
    "pipeline = make_pipeline(StandardScaler(), model)\n",
    "cv_recall_scores = cross_val_score(pipeline, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# --- Step 4: Save performance metrics\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Metrics saved to {output_csv_path}\")\n",
    "\n",
    "# --- Step 5: Permutation feature importance\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test_input, y_test_input, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose: model title as row, features as columns\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba91bf-b1d6-4a95-bbd5-a0f1a1cc8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM + GridSearchCV\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ðŸ” Update this for each run\n",
    "model_title = \"SVM_gridsearch_original\"  # Change this title for each run\n",
    "\n",
    "# ðŸ§  Define your data inputs (can change this per run)\n",
    "X_train_input = X_train\n",
    "y_train_input = y_train\n",
    "X_test_input  = X_test\n",
    "y_test_input  = y_test\n",
    "\n",
    "# Step 1: GridSearchCV setup\n",
    "svm_base = SVC(probability=True, class_weight='balanced', random_state=42)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 0.01, 0.1],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='precision',  # Optimizing for precision on positive class\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Step 2: Train and predict\n",
    "grid_search.fit(X_train_input, y_train_input)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"âœ… Best params: {grid_search.best_params_}\")\n",
    "\n",
    "y_pred = best_model.predict(X_test_input)\n",
    "y_proba = best_model.predict_proba(X_test_input)[:, 1]\n",
    "\n",
    "# Step 3: Evaluation metrics\n",
    "accuracy = accuracy_score(y_test_input, y_pred)\n",
    "recall = recall_score(y_test_input, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_input, y_pred)\n",
    "auc = roc_auc_score(y_test_input, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_input, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "# Step 4: Cross-validated recall (on full original data for fairness)\n",
    "pipeline = make_pipeline(StandardScaler(), best_model)\n",
    "cv_recall_scores = cross_val_score(pipeline, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# Step 5: Save evaluation metrics\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# Step 6: Permutation feature importance\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test_input, y_test_input, n_repeats=10,\n",
    "    random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "# Append or create feature importance CSV\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7132d3-1ab3-456e-a521-3dc2aa9d9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SVM with Scaled Data and GridSearch ===\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Model title\n",
    "model_title = \"SVM_scaled_GridSearch\"\n",
    "\n",
    "# --- Prepare data\n",
    "X_train_input = X_train\n",
    "X_test_input = X_test\n",
    "y_train_input = y_train\n",
    "y_test_input = y_test\n",
    "\n",
    "# --- Define pipeline with scaler and SVM\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# --- Define parameter grid for SVM inside pipeline\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__gamma': ['scale', 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train_input, y_train_input)\n",
    "\n",
    "# --- Best model\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# --- Predict\n",
    "y_pred = best_model.predict(X_test_input)\n",
    "y_proba = best_model.predict_proba(X_test_input)[:, 1]\n",
    "\n",
    "# --- Metrics\n",
    "accuracy = accuracy_score(y_test_input, y_pred)\n",
    "recall = recall_score(y_test_input, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_input, y_pred)\n",
    "auc = roc_auc_score(y_test_input, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_input, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "# --- Cross-validated recall (for comparison)\n",
    "cv_recall_scores = cross_val_score(best_model, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# --- Save model evaluation\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# --- Permutation Feature Importance\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test_input, y_test_input, n_repeats=10, random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose and save\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1),\n",
    "    columns=feature_names,\n",
    "    index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9cb1a-a41e-4129-a080-d0394fddd23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM smote+grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Model title\n",
    "model_title = \"SVM_SMOTE_GridSearch\"\n",
    "\n",
    "# --- Apply SMOTE to training data (no scaling)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# --- Define pipeline (NO SCALER)\n",
    "pipeline = Pipeline([\n",
    "    ('svm', SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# --- Define parameter grid for SVM\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__gamma': ['scale', 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# --- Grid Search\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# --- Best model\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# --- Predict\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# --- Cross-validated recall (on original data for comparison)\n",
    "cv_recall_scores = cross_val_score(best_model, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# --- Save model evaluation\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# --- Permutation Feature Importance\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose and save\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1),\n",
    "    columns=feature_names,\n",
    "    index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675d71c-8706-4ed2-b9a9-597ab14ab2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM scaled+smote+gridseaerch \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline  # imblearn's pipeline, not sklearn's\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Model title\n",
    "model_title = \"SVM_SMOTE_Scaled_GridSearch\"\n",
    "\n",
    "# --- Define pipeline with scaling + SMOTE + SVM\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('svm', SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# --- Define parameter grid\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__gamma': ['scale', 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# --- Grid Search\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# --- Best model\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# --- Predict\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# --- Cross-validated recall on original data\n",
    "cv_recall_scores = cross_val_score(best_model, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# --- Save model evaluation\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# --- Permutation Feature Importance\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose and save\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1),\n",
    "    columns=feature_names,\n",
    "    index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1a87c3-142a-44e3-8fd7-016445590dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd10b1-3bba-4b4b-be3f-22bdc641ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Decision Tree Classification ===\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"DecisionTree_original\"  # ðŸ” Change this for each run\n",
    "\n",
    "# === Train the Decision Tree ===\n",
    "model = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72537ba3-194a-4b0e-a8e5-158d43dd31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree scaled\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"DecisionTree_Scaled_NoSMOTE_NoGridSearch\"\n",
    "\n",
    "# === Define pipeline with scaler and Decision Tree ===\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# === Train model ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee8088-2738-4331-834a-968cc5b8e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree smote\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"DecisionTree_SMOTE_NoGridSearch_NoScaling\"\n",
    "\n",
    "# === Apply SMOTE to training data ===\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# === Define and train model ===\n",
    "model = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall (using resampled training data) ===\n",
    "cv_recall_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763f7b1-b9fc-40d0-b2cb-a46dc03eccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decisioon Tree grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"DecisionTree_GridSearch_NoSMOTE_NoScaling\"\n",
    "\n",
    "# === Define classifier ===\n",
    "clf = DecisionTreeClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# === Define parameter grid for GridSearch ===\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# === Run GridSearchCV ===\n",
    "grid = GridSearchCV(clf, param_grid, cv=3, scoring='recall', n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a800cd-b508-4c9e-a228-d3ddb71682f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree scaled +grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline  # sklearn pipeline\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"DecisionTree_Scaled_GridSearch\"\n",
    "\n",
    "# === Define pipeline: Scaling -> Decision Tree (No SMOTE)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# === Define parameter grid for GridSearch ===\n",
    "param_grid = {\n",
    "    'clf__max_depth': [None, 5, 10, 20],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# === Run GridSearchCV ===\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4101f171-9076-411d-88db-0808a18e78ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree smote +grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Use imblearn pipeline\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"DecisionTree_SMOTE_GridSearch\"\n",
    "\n",
    "# === Define pipeline: SMOTE -> Decision Tree (NO SCALING)\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# === Define parameter grid for GridSearch ===\n",
    "param_grid = {\n",
    "    'clf__max_depth': [None, 5, 10, 20],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# === Run GridSearchCV ===\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915c0d5-fda7-4fa4-881b-8103a488d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree scaled +smote +gridsearch \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # imblearn's pipeline\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"DecisionTree_Scaled_SMOTE_GridSearch\"\n",
    "\n",
    "# === Define pipeline: Scaling -> SMOTE -> Decision Tree ===\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('clf', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# === Define parameter grid for GridSearch ===\n",
    "param_grid = {\n",
    "    'clf__max_depth': [None, 5, 10, 20],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# === Run GridSearchCV ===\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3b319-dc10-4b5c-88f4-3d3591e6be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd85776-6dad-4ee5-9865-3f128e27ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF standard\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier  # <-- Changed here\n",
    "\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"RandomForest_original\"  # Changed model title accordingly\n",
    "\n",
    "# === Train the Random Forest ===\n",
    "model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b51c17-5ae1-4724-adc6-f0eb0a79e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF scaled\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"RandomForest_scaled\"\n",
    "\n",
    "# === Create pipeline with scaler and RF ===\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# === Train model using pipeline ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcfa613-5a5d-450b-bfdd-d8beef15ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF Smote\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline  # imblearn's pipeline to handle SMOTE\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"RandomForest_SMOTE\"\n",
    "\n",
    "# === Create pipeline with SMOTE and Random Forest ===\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# === Train model using pipeline ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c11df-7662-48ab-82b5-f29518f9d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"RandomForest_GridSearch\"\n",
    "\n",
    "# === Define Random Forest model ===\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# === Define parameter grid for GridSearch ===\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# === GridSearchCV ===\n",
    "grid = GridSearchCV(rf, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789bf745-fb46-4bf9-9964-2af4480e4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RF smote +grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"RandomForest_SMOTE_GridSearch\"\n",
    "\n",
    "# === Define pipeline with SMOTE and Random Forest (no scaler) ===\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# === Define parameter grid for GridSearch ===\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'rf__max_depth': [None, 5, 10, 20],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# === GridSearchCV setup ===\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3abfc-abf5-4574-a561-1d0ebf7f9d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ed0b6-0f74-46bb-b0d0-c937f3d6036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network Standard\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NeuralNetwork_original\"  # Updated model title\n",
    "\n",
    "# === Train the Neural Network ===\n",
    "model = MLPClassifier(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "# MLPClassifier has predict_proba only if 'solver' supports it (default 'adam' supports it)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200f12a-063f-4bdd-b78b-e1b8abcaea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN with scaler\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NeuralNetwork_Scaled\"\n",
    "\n",
    "# === Define pipeline: Scaling + Neural Network ===\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('mlp', MLPClassifier(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# === Fit model ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19014684-681d-4cdf-8415-47b2f644aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN smote\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NeuralNetwork_SMOTE\"\n",
    "\n",
    "# === Define pipeline: SMOTE + Neural Network (no scaler) ===\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('mlp', MLPClassifier(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# === Fit model ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bbd54f-828d-43ee-96c6-cafe44c5f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN grid search  \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NeuralNetwork_GridSearch\"\n",
    "\n",
    "# === Define model and parameter grid ===\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=MLPClassifier(random_state=42, max_iter=1000),\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# === Fit model ===\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92dd741-1672-4f6c-b7d3-1949f6aa951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN grid search +smote\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NeuralNetwork_SMOTE_GridSearch\"\n",
    "\n",
    "# === Define pipeline: SMOTE + Neural Network ===\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('nn', MLPClassifier(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# === Define parameter grid for GridSearch ===\n",
    "param_grid = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01],\n",
    "    'nn__solver': ['adam', 'lbfgs'],\n",
    "    'nn__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# === Fit model ===\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364a74f-621a-4080-bbe2-9c7d4a4c1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN Scaled and grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NeuralNetwork_Scaled_GridSearch\"\n",
    "\n",
    "# === Define pipeline: Scaling -> Neural Network ===\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nn', MLPClassifier(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# === Define parameter grid ===\n",
    "param_grid = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01],\n",
    "    'nn__solver': ['adam', 'lbfgs'],\n",
    "    'nn__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# === Fit model ===\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fc85a-6974-4a41-96f4-7d5b5445c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN scaled +smote+gridsearch\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NeuralNetwork_Scaled_SMOTE_GridSearch\"\n",
    "\n",
    "# === Define pipeline: Scaling -> SMOTE -> Neural Network ===\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('nn', MLPClassifier(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# === Define parameter grid ===\n",
    "param_grid = {\n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'nn__alpha': [0.0001, 0.001, 0.01],\n",
    "    'nn__solver': ['adam', 'lbfgs'],\n",
    "    'nn__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# === Grid Search ===\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# === Fit model ===\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbef2b4-110f-4c1e-9488-54e1c1ff60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fea51e-3992-4355-a34e-c52e9e31a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost standard\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Model title for tracking\n",
    "model_title = \"XGBoost_original\"\n",
    "\n",
    "# --- Use raw train/test sets\n",
    "X_train_input = X_train\n",
    "y_train_input = y_train\n",
    "X_test_input  = X_test\n",
    "y_test_input  = y_test\n",
    "\n",
    "# --- Initialize XGBoost\n",
    "model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --- Train and Predict\n",
    "model.fit(X_train_input, y_train_input)\n",
    "y_pred = model.predict(X_test_input)\n",
    "y_proba = model.predict_proba(X_test_input)[:, 1]\n",
    "\n",
    "# --- Metrics\n",
    "accuracy = accuracy_score(y_test_input, y_pred)\n",
    "recall = recall_score(y_test_input, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_input, y_pred)\n",
    "auc = roc_auc_score(y_test_input, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_input, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "\n",
    "# --- Cross-validated recall (on full dataset)\n",
    "pipeline = make_pipeline(model)\n",
    "cv_recall_scores = cross_val_score(pipeline, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# --- Save model evaluation\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# --- Permutation Feature Importance\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test_input, y_test_input, n_repeats=10, random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose for saving\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1),\n",
    "    columns=feature_names,\n",
    "    index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d77c8-d638-42d0-9208-cd9d6733e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost scaled\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"XGBoost_Scaled\"\n",
    "\n",
    "# === Define pipeline: Scaling -> XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=1,  # Adjust if there's class imbalance\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Fit model ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5823ab35-eb69-4204-85b6-54ddcba6580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost smote\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline  # imblearn's pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"XGBoost_SMOTE\"\n",
    "\n",
    "# === Define pipeline: SMOTE -> XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('xgb', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=1,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Fit model ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523ca6b-0305-46ed-abab-e6b120b544c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost scaled and smote\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline  # imblearn's pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"XGBoost_SMOTE_Scaled\"\n",
    "\n",
    "# === Define pipeline: SMOTE -> Scaling -> XGBoost\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        scale_pos_weight=1,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Fit model ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    pipeline, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c095e-7ce0-471c-ba72-0edcad0b3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost gridsearch\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Model title\n",
    "model_title = \"XGBoost_original_GridSearch\"\n",
    "\n",
    "# --- Use unscaled, unbalanced data\n",
    "X_train_input = X_train\n",
    "y_train_input = y_train\n",
    "X_test_input  = X_test\n",
    "y_test_input  = y_test\n",
    "\n",
    "# --- Define base model and parameter grid\n",
    "xgb_base = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(xgb_base, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train_input, y_train_input)\n",
    "\n",
    "# --- Get best model\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model params: {grid.best_params_}\")\n",
    "\n",
    "# --- Predict with best model\n",
    "y_pred = best_model.predict(X_test_input)\n",
    "y_proba = best_model.predict_proba(X_test_input)[:, 1]\n",
    "\n",
    "# --- Metrics\n",
    "accuracy = accuracy_score(y_test_input, y_pred)\n",
    "recall = recall_score(y_test_input, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_input, y_pred)\n",
    "auc = roc_auc_score(y_test_input, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_input, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "\n",
    "# --- Cross-validated recall (on full dataset)\n",
    "pipeline = make_pipeline(best_model)\n",
    "cv_recall_scores = cross_val_score(pipeline, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# --- Save model evaluation\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# --- Permutation Feature Importance\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test_input, y_test_input, n_repeats=10, random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose and save\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1),\n",
    "    columns=feature_names,\n",
    "    index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb18dc6f-db3e-411e-8510-d54f8ce10f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost scaled grid search \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Model title\n",
    "model_title = \"XGBoost_scaled_GridSearch\"\n",
    "\n",
    "# --- Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_input = scaler.fit_transform(X_train)\n",
    "X_test_input = scaler.transform(X_test)\n",
    "y_train_input = y_train\n",
    "y_test_input = y_test\n",
    "\n",
    "# --- Define base model and parameter grid\n",
    "xgb_base = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(xgb_base, param_grid, cv=3, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X_train_input, y_train_input)\n",
    "\n",
    "# --- Get best model\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model params: {grid.best_params_}\")\n",
    "\n",
    "# --- Predict with best model\n",
    "y_pred = best_model.predict(X_test_input)\n",
    "y_proba = best_model.predict_proba(X_test_input)[:, 1]\n",
    "\n",
    "# --- Metrics\n",
    "accuracy = accuracy_score(y_test_input, y_pred)\n",
    "recall = recall_score(y_test_input, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_input, y_pred)\n",
    "auc = roc_auc_score(y_test_input, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_input, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "# --- Cross-validated recall (on full unscaled dataset for fair comp)\n",
    "pipeline = make_pipeline(StandardScaler(), best_model)\n",
    "cv_recall_scores = cross_val_score(pipeline, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# --- Save model evaluation\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# --- Permutation Feature Importance\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test_input, y_test_input, n_repeats=10, random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# ðŸ”„ Transpose and save\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1),\n",
    "    columns=feature_names,\n",
    "    index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241803c0-90ee-442f-999f-7e186e63d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost smote grid search\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ðŸ” Update this for each run\n",
    "model_title = \"XGBoost_gridsearch_scaled_SMOTE\"  # Change title for each run\n",
    "\n",
    "# ðŸ§  Define your input data variant\n",
    "X_train_input = X_train_resampled   # e.g., X_train, X_train_scaled, X_train_resampled\n",
    "y_train_input = y_train_resampled\n",
    "X_test_input  = X_test_scaled       # e.g., X_test or X_test_scaled\n",
    "y_test_input  = y_test\n",
    "\n",
    "# Step 1: GridSearchCV\n",
    "xgb_base = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    scale_pos_weight=1  # adjust if imbalance is extreme (or skip if using SMOTE)\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring='precision',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Step 2: Train model and predict\n",
    "grid_search.fit(X_train_input, y_train_input)\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"âœ… Best XGBoost Params: {grid_search.best_params_}\")\n",
    "\n",
    "y_pred = best_model.predict(X_test_input)\n",
    "y_proba = best_model.predict_proba(X_test_input)[:, 1]\n",
    "\n",
    "# Step 3: Evaluation metrics\n",
    "accuracy = accuracy_score(y_test_input, y_pred)\n",
    "recall = recall_score(y_test_input, y_pred)\n",
    "mcc = matthews_corrcoef(y_test_input, y_pred)\n",
    "auc = roc_auc_score(y_test_input, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_input, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test_input, y_pred)\n",
    "\n",
    "# Step 4: Cross-validated recall (on full dataset, no resampling)\n",
    "pipeline = make_pipeline(StandardScaler(), best_model)\n",
    "cv_recall_scores = cross_val_score(pipeline, X, y, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# Step 5: Save metrics to CSV\n",
    "header = [\"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\", \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"]\n",
    "row = [model_title, accuracy, recall, specificity, auc, mcc, cv_recall_mean, f1]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Evaluation metrics saved to {output_csv_path}\")\n",
    "\n",
    "# Step 6: Feature importance via permutation\n",
    "print(f\"\\nðŸ“Š Computing permutation importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test_input, y_test_input, n_repeats=10,\n",
    "    random_state=42, scoring='precision'\n",
    ")\n",
    "\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "# Save to feature importance matrix\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763ab42-ff5f-4f39-ac06-3bd08ebb0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost scaled, smote and gridsearch \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline  # imblearn pipeline for SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"XGBoost_SMOTE_Scaled_GridSearch\"\n",
    "\n",
    "# === Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Define parameter grid\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# === GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0dcf0-2518-4fe8-bfe2-dbe2f0acc8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d99a0b3a-9d8f-4bc6-a85e-c50eb5909556",
   "metadata": {},
   "source": [
    "#### Naive Bayers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6811b47-93eb-4f67-b281-29002b68061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayers standard \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NaiveBayes_original\"  # Updated model title\n",
    "\n",
    "# === Train the Naive Bayes model ===\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665d936-37ef-4d6f-a2c9-b3ee6bd8d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB Scaled\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NaiveBayes_Scaled\"  # Updated model title\n",
    "\n",
    "# === Scale features ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === Train the Naive Bayes model ===\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall (on scaled data) ===\n",
    "cv_recall_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test_scaled, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d1313-577e-44ad-8688-5ff335ed8d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB with smote\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.over_sampling import SMOTE  # âœ… Added SMOTE\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NaiveBayes_SMOTE\"\n",
    "\n",
    "# === Apply SMOTE ===\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# === Train the Naive Bayes model ===\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall (on original data for comparison) ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9222fef2-6196-4873-8fa8-73d10e05ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB Gridsearch \n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NaiveBayes_GridSearch\"\n",
    "\n",
    "# === Define parameter grid ===\n",
    "param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "# === GridSearchCV ===\n",
    "grid = GridSearchCV(\n",
    "    estimator=GaussianNB(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred, pos_label=1)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e189e-7a18-41e1-8a81-af258c386ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB with smote and gridsearch\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NaiveBayes_SMOTE_GridSearch\"\n",
    "\n",
    "# === Define parameter grid ===\n",
    "param_grid = {\n",
    "    'model__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "# === Build pipeline: SMOTE + Naive Bayes ===\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', GaussianNB())\n",
    "])\n",
    "\n",
    "# === Grid Search ===\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fae4d-2abd-42a5-a961-3487ae1baf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb  scaled and gridsearch\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NaiveBayes_Scaled_GridSearch\"\n",
    "\n",
    "# === Define parameter grid ===\n",
    "param_grid = {\n",
    "    'model__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "# === Build pipeline: Scaling + Naive Bayes ===\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', GaussianNB())\n",
    "])\n",
    "\n",
    "# === Grid Search ===\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a7dcc-13dd-4bc7-be96-ed5402487d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nb with scaling smote and gridsearch\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, confusion_matrix,\n",
    "    roc_auc_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# === Set model title ===\n",
    "model_title = \"NaiveBayes_Scaled_SMOTE_GridSearch\"\n",
    "\n",
    "# === Define pipeline: Scaling -> SMOTE -> Naive Bayes ===\n",
    "pipeline = ImbPipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', GaussianNB())\n",
    "])\n",
    "\n",
    "# === Define parameter grid ===\n",
    "param_grid = {\n",
    "    'model__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "}\n",
    "\n",
    "# === Grid Search ===\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='recall',\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# === Best model ===\n",
    "best_model = grid.best_estimator_\n",
    "print(f\"âœ… Best model parameters: {grid.best_params_}\")\n",
    "\n",
    "# === Predictions ===\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# === Metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# === Cross-validated Recall ===\n",
    "cv_recall_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='recall')\n",
    "cv_recall_mean = cv_recall_scores.mean()\n",
    "\n",
    "# === Save metrics to CSV ===\n",
    "header = [\n",
    "    \"Model_Title\", \"Accuracy\", \"Recall\", \"Specificity\",\n",
    "    \"AUC\", \"MCC\", \"CV_Recall_Mean\", \"F1\"\n",
    "]\n",
    "\n",
    "row = [\n",
    "    model_title, accuracy, recall, specificity,\n",
    "    auc, mcc, cv_recall_mean, f1\n",
    "]\n",
    "\n",
    "file_exists = os.path.isfile(output_csv_path)\n",
    "\n",
    "with open(output_csv_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(header)\n",
    "    writer.writerow(row)\n",
    "\n",
    "print(f\"âœ… Results saved to {output_csv_path}\")\n",
    "\n",
    "# === Permutation Feature Importance ===\n",
    "print(f\"\\nðŸ“Š Computing feature importances for: {model_title}\")\n",
    "result = permutation_importance(\n",
    "    best_model, X_test, y_test, n_repeats=10, random_state=42, scoring='recall'\n",
    ")\n",
    "importances = pd.Series(result.importances_mean, index=feature_names)\n",
    "\n",
    "# === Save feature importances ===\n",
    "importance_df = pd.DataFrame(\n",
    "    importances.values.reshape(1, -1), columns=feature_names, index=[model_title]\n",
    ")\n",
    "\n",
    "if os.path.exists(feature_importance_csv):\n",
    "    existing_df = pd.read_csv(feature_importance_csv, index_col=0)\n",
    "    updated_df = pd.concat([existing_df, importance_df])\n",
    "else:\n",
    "    updated_df = importance_df\n",
    "\n",
    "updated_df.to_csv(feature_importance_csv)\n",
    "print(f\"âœ… Feature importances saved to {feature_importance_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594c412-7f9c-44a6-bb03-883a42a3f4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
