{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4429062d-6386-4c22-b068-fb1956ab9f98",
   "metadata": {},
   "source": [
    "# Comparing initial prompt outputs. Update API key accordingly and output file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256b5a5-c577-4880-a10b-56fccb9c7202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ecfeb-935a-497b-8955-1936bc42e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output1L.txt\", \"Output1M.txt\", \"Output1D.txt\", \"Output1G.txt\",\n",
    "    \"Output2L.txt\", \"Output2M.txt\", \"Output2D.txt\", \"Output2G.txt\",\n",
    "    \"Output3L.txt\", \"Output3M.txt\", \"Output3D.txt\", \"Output3G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "\n",
    "# === 8. Aggregate Statistics (Frequency + Rank) ===\n",
    "feature_stats = defaultdict(lambda: {\"files\": set(), \"ranks\": []})\n",
    "\n",
    "for fname, features in normalized_outputs.items():\n",
    "    for rank, feat in enumerate(features, start=1):\n",
    "        clean_feat = feat.lower().strip()\n",
    "        feature_stats[clean_feat][\"files\"].add(fname)\n",
    "        feature_stats[clean_feat][\"ranks\"].append(rank)\n",
    "\n",
    "# === 9. Compute Aggregates ===\n",
    "summary = []\n",
    "for feat, data in feature_stats.items():\n",
    "    num_files = len(data[\"files\"])\n",
    "    avg_rank = statistics.mean(data[\"ranks\"])\n",
    "    median_rank = statistics.median(data[\"ranks\"])\n",
    "    most_common_rank = Counter(data[\"ranks\"]).most_common(1)[0][0]\n",
    "    summary.append({\n",
    "        \"Feature\": feat,\n",
    "        \"Files Mentioned\": num_files,\n",
    "        \"Average Rank\": round(avg_rank, 2),\n",
    "        \"Median Rank\": round(median_rank, 2),\n",
    "        \"Most Common Rank\": most_common_rank,\n",
    "        \"Files\": \"; \".join(sorted(data[\"files\"]))\n",
    "    })\n",
    "\n",
    "# === 10. Sort & Save ===\n",
    "summary_sorted = sorted(summary, key=lambda x: (-x[\"Files Mentioned\"], x[\"Average Rank\"]))\n",
    "csv_path = os.path.join(folder_path, \"Feature_Frequency_and_Rank_Normalized.csv\")\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=summary_sorted[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(summary_sorted)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved normalized feature frequency analysis: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top 10 ===\n",
    "print(\"\\n=== Top 10 Most Common (Normalized) Features ===\")\n",
    "for item in summary_sorted[:10]:\n",
    "    print(f\"- {item['Feature']} ‚Üí in {item['Files Mentioned']} files (avg rank {item['Average Rank']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5dd514-20c4-417b-be00-bb9e75f65a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output4L.txt\", \"Output4M.txt\", \"Output4D.txt\", \"Output4G.txt\",\n",
    "    \"Output5L.txt\", \"Output5M.txt\", \"Output5D.txt\", \"Output5G.txt\",\n",
    "    \"Output6L.txt\", \"Output6M.txt\", \"Output6D.txt\", \"Output6G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "\n",
    "# === 8. Aggregate Statistics (Frequency + Rank) ===\n",
    "feature_stats = defaultdict(lambda: {\"files\": set(), \"ranks\": []})\n",
    "\n",
    "for fname, features in normalized_outputs.items():\n",
    "    for rank, feat in enumerate(features, start=1):\n",
    "        clean_feat = feat.lower().strip()\n",
    "        feature_stats[clean_feat][\"files\"].add(fname)\n",
    "        feature_stats[clean_feat][\"ranks\"].append(rank)\n",
    "\n",
    "# === 9. Compute Aggregates ===\n",
    "summary = []\n",
    "for feat, data in feature_stats.items():\n",
    "    num_files = len(data[\"files\"])\n",
    "    avg_rank = statistics.mean(data[\"ranks\"])\n",
    "    median_rank = statistics.median(data[\"ranks\"])\n",
    "    most_common_rank = Counter(data[\"ranks\"]).most_common(1)[0][0]\n",
    "    summary.append({\n",
    "        \"Feature\": feat,\n",
    "        \"Files Mentioned\": num_files,\n",
    "        \"Average Rank\": round(avg_rank, 2),\n",
    "        \"Median Rank\": round(median_rank, 2),\n",
    "        \"Most Common Rank\": most_common_rank,\n",
    "        \"Files\": \"; \".join(sorted(data[\"files\"]))\n",
    "    })\n",
    "\n",
    "# === 10. Sort & Save ===\n",
    "summary_sorted = sorted(summary, key=lambda x: (-x[\"Files Mentioned\"], x[\"Average Rank\"]))\n",
    "csv_path = os.path.join(folder_path, \"Feature_Frequency_and_Rank_Normalized4-6.csv\")\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=summary_sorted[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(summary_sorted)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved normalized feature frequency analysis: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top 10 ===\n",
    "print(\"\\n=== Top 10 Most Common (Normalized) Features ===\")\n",
    "for item in summary_sorted[:10]:\n",
    "    print(f\"- {item['Feature']} ‚Üí in {item['Files Mentioned']} files (avg rank {item['Average Rank']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d4d03-d698-4729-a66f-e5ec93a288c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output7L.txt\", \"Output7M.txt\", \"Output7D.txt\", \"Output7G.txt\",\n",
    "    \"Output8L.txt\", \"Output8M.txt\", \"Output8D.txt\", \"Output8G.txt\",\n",
    "    \"Output9L.txt\", \"Output9M.txt\", \"Output9D.txt\", \"Output9G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "\n",
    "# === 8. Aggregate Statistics (Frequency + Rank) ===\n",
    "feature_stats = defaultdict(lambda: {\"files\": set(), \"ranks\": []})\n",
    "\n",
    "for fname, features in normalized_outputs.items():\n",
    "    for rank, feat in enumerate(features, start=1):\n",
    "        clean_feat = feat.lower().strip()\n",
    "        feature_stats[clean_feat][\"files\"].add(fname)\n",
    "        feature_stats[clean_feat][\"ranks\"].append(rank)\n",
    "\n",
    "# === 9. Compute Aggregates ===\n",
    "summary = []\n",
    "for feat, data in feature_stats.items():\n",
    "    num_files = len(data[\"files\"])\n",
    "    avg_rank = statistics.mean(data[\"ranks\"])\n",
    "    median_rank = statistics.median(data[\"ranks\"])\n",
    "    most_common_rank = Counter(data[\"ranks\"]).most_common(1)[0][0]\n",
    "    summary.append({\n",
    "        \"Feature\": feat,\n",
    "        \"Files Mentioned\": num_files,\n",
    "        \"Average Rank\": round(avg_rank, 2),\n",
    "        \"Median Rank\": round(median_rank, 2),\n",
    "        \"Most Common Rank\": most_common_rank,\n",
    "        \"Files\": \"; \".join(sorted(data[\"files\"]))\n",
    "    })\n",
    "\n",
    "# === 10. Sort & Save ===\n",
    "summary_sorted = sorted(summary, key=lambda x: (-x[\"Files Mentioned\"], x[\"Average Rank\"]))\n",
    "csv_path = os.path.join(folder_path, \"Feature_Frequency_and_Rank_Normalized7-9.csv\")\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=summary_sorted[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(summary_sorted)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved normalized feature frequency analysis: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top 10 ===\n",
    "print(\"\\n=== Top 10 Most Common (Normalized) Features ===\")\n",
    "for item in summary_sorted[:10]:\n",
    "    print(f\"- {item['Feature']} ‚Üí in {item['Files Mentioned']} files (avg rank {item['Average Rank']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e9a8f-22b5-488e-9fed-e4476d694f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output10L.txt\", \"Output10M.txt\", \"Output10D.txt\", \"Output10G.txt\",\n",
    "    \"Output11L.txt\", \"Output11M.txt\", \"Output11D.txt\", \"Output11G.txt\",\n",
    "    \"Output12L.txt\", \"Output12M.txt\", \"Output12D.txt\", \"Output12G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "\n",
    "# === 8. Aggregate Statistics (Frequency + Rank) ===\n",
    "feature_stats = defaultdict(lambda: {\"files\": set(), \"ranks\": []})\n",
    "\n",
    "for fname, features in normalized_outputs.items():\n",
    "    for rank, feat in enumerate(features, start=1):\n",
    "        clean_feat = feat.lower().strip()\n",
    "        feature_stats[clean_feat][\"files\"].add(fname)\n",
    "        feature_stats[clean_feat][\"ranks\"].append(rank)\n",
    "\n",
    "# === 9. Compute Aggregates ===\n",
    "summary = []\n",
    "for feat, data in feature_stats.items():\n",
    "    num_files = len(data[\"files\"])\n",
    "    avg_rank = statistics.mean(data[\"ranks\"])\n",
    "    median_rank = statistics.median(data[\"ranks\"])\n",
    "    most_common_rank = Counter(data[\"ranks\"]).most_common(1)[0][0]\n",
    "    summary.append({\n",
    "        \"Feature\": feat,\n",
    "        \"Files Mentioned\": num_files,\n",
    "        \"Average Rank\": round(avg_rank, 2),\n",
    "        \"Median Rank\": round(median_rank, 2),\n",
    "        \"Most Common Rank\": most_common_rank,\n",
    "        \"Files\": \"; \".join(sorted(data[\"files\"]))\n",
    "    })\n",
    "\n",
    "# === 10. Sort & Save ===\n",
    "summary_sorted = sorted(summary, key=lambda x: (-x[\"Files Mentioned\"], x[\"Average Rank\"]))\n",
    "csv_path = os.path.join(folder_path, \"Feature_Frequency_and_Rank_Normalized10-12.csv\")\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=summary_sorted[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(summary_sorted)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved normalized feature frequency analysis: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top 10 ===\n",
    "print(\"\\n=== Top 10 Most Common (Normalized) Features ===\")\n",
    "for item in summary_sorted[:10]:\n",
    "    print(f\"- {item['Feature']} ‚Üí in {item['Files Mentioned']} files (avg rank {item['Average Rank']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893faedb-40f5-4150-98df-bf648836804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output13L.txt\", \"Output13M.txt\", \"Output13D.txt\", \"Output13G.txt\",\n",
    "    \"Output14L.txt\", \"Output14M.txt\", \"Output14D.txt\", \"Output14G.txt\",\n",
    "    \"Output15L.txt\", \"Output15M.txt\", \"Output15D.txt\", \"Output15G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash\")\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "\n",
    "# === 8. Aggregate Statistics (Frequency + Rank) ===\n",
    "feature_stats = defaultdict(lambda: {\"files\": set(), \"ranks\": []})\n",
    "\n",
    "for fname, features in normalized_outputs.items():\n",
    "    for rank, feat in enumerate(features, start=1):\n",
    "        clean_feat = feat.lower().strip()\n",
    "        feature_stats[clean_feat][\"files\"].add(fname)\n",
    "        feature_stats[clean_feat][\"ranks\"].append(rank)\n",
    "\n",
    "# === 9. Compute Aggregates ===\n",
    "summary = []\n",
    "for feat, data in feature_stats.items():\n",
    "    num_files = len(data[\"files\"])\n",
    "    avg_rank = statistics.mean(data[\"ranks\"])\n",
    "    median_rank = statistics.median(data[\"ranks\"])\n",
    "    most_common_rank = Counter(data[\"ranks\"]).most_common(1)[0][0]\n",
    "    summary.append({\n",
    "        \"Feature\": feat,\n",
    "        \"Files Mentioned\": num_files,\n",
    "        \"Average Rank\": round(avg_rank, 2),\n",
    "        \"Median Rank\": round(median_rank, 2),\n",
    "        \"Most Common Rank\": most_common_rank,\n",
    "        \"Files\": \"; \".join(sorted(data[\"files\"]))\n",
    "    })\n",
    "\n",
    "# === 10. Sort & Save ===\n",
    "summary_sorted = sorted(summary, key=lambda x: (-x[\"Files Mentioned\"], x[\"Average Rank\"]))\n",
    "csv_path = os.path.join(folder_path, \"Feature_Frequency_and_Rank_Normalized13-15.csv\")\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=summary_sorted[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(summary_sorted)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved normalized feature frequency analysis: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top 10 ===\n",
    "print(\"\\n=== Top 10 Most Common (Normalized) Features ===\")\n",
    "for item in summary_sorted[:10]:\n",
    "    print(f\"- {item['Feature']} ‚Üí in {item['Files Mentioned']} files (avg rank {item['Average Rank']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368b561-1b87-40ed-934f-9b5c86793a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switched to different gemini model to avoid timing out \n",
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output1L.txt\", \"Output1M.txt\", \"Output1D.txt\", \"Output1G.txt\",\n",
    "    \"Output2L.txt\", \"Output2M.txt\", \"Output2D.txt\", \"Output2G.txt\",\n",
    "    \"Output3L.txt\", \"Output3M.txt\", \"Output3D.txt\", \"Output3G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-lite\")\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "    \n",
    "# === 8. Rank-Weighted Similarity Function ===\n",
    "def rank_weighted_similarity(list1, list2):\n",
    "    \"\"\"Compute similarity score accounting for feature rank (higher ranks weigh more).\"\"\"\n",
    "    max_len = max(len(list1), len(list2))\n",
    "    score = 0.0\n",
    "    for i, f1 in enumerate(list1):\n",
    "        for j, f2 in enumerate(list2):\n",
    "            if f1 == f2:\n",
    "                # Weight: closer to top = higher weight\n",
    "                weight = 1 / (1 + abs(i - j))\n",
    "                score += weight\n",
    "    # Normalize by average length to keep 0-1 scale\n",
    "    return score / max_len\n",
    "\n",
    "# === 9. Compute Similarity Matrix ===\n",
    "file_list = sorted(normalized_outputs.keys())\n",
    "similarity_matrix = []\n",
    "\n",
    "for f1 in file_list:\n",
    "    row = [f1]\n",
    "    for f2 in file_list:\n",
    "        sim = rank_weighted_similarity(normalized_outputs[f1], normalized_outputs[f2])\n",
    "        row.append(round(sim, 3))\n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "# === 10. Save Similarity Matrix to CSV ===\n",
    "csv_path = os.path.join(folder_path, \"Feature_List_Similarity_Matrix1-3.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"File\"] + file_list)\n",
    "    writer.writerows(similarity_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved rank-weighted similarity matrix: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top Similarities ===\n",
    "print(\"\\n=== Sample Similarities ===\")\n",
    "for row in similarity_matrix[:5]:\n",
    "    print(row[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfed02-e34d-4a8d-8d70-d7fa112a1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output4L.txt\", \"Output4M.txt\", \"Output4D.txt\", \"Output4G.txt\",\n",
    "    \"Output5L.txt\", \"Output5M.txt\", \"Output5D.txt\", \"Output5G.txt\",\n",
    "    \"Output6L.txt\", \"Output6M.txt\", \"Output6D.txt\", \"Output6G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-lite\")\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "    \n",
    "# === 8. Rank-Weighted Similarity Function ===\n",
    "def rank_weighted_similarity(list1, list2):\n",
    "    \"\"\"Compute similarity score accounting for feature rank (higher ranks weigh more).\"\"\"\n",
    "    max_len = max(len(list1), len(list2))\n",
    "    score = 0.0\n",
    "    for i, f1 in enumerate(list1):\n",
    "        for j, f2 in enumerate(list2):\n",
    "            if f1 == f2:\n",
    "                # Weight: closer to top = higher weight\n",
    "                weight = 1 / (1 + abs(i - j))\n",
    "                score += weight\n",
    "    # Normalize by average length to keep 0-1 scale\n",
    "    return score / max_len\n",
    "\n",
    "# === 9. Compute Similarity Matrix ===\n",
    "file_list = sorted(normalized_outputs.keys())\n",
    "similarity_matrix = []\n",
    "\n",
    "for f1 in file_list:\n",
    "    row = [f1]\n",
    "    for f2 in file_list:\n",
    "        sim = rank_weighted_similarity(normalized_outputs[f1], normalized_outputs[f2])\n",
    "        row.append(round(sim, 3))\n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "# === 10. Save Similarity Matrix to CSV ===\n",
    "csv_path = os.path.join(folder_path, \"Feature_List_Similarity_Matrix4-6.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"File\"] + file_list)\n",
    "    writer.writerows(similarity_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved rank-weighted similarity matrix: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top Similarities ===\n",
    "print(\"\\n=== Sample Similarities ===\")\n",
    "for row in similarity_matrix[:5]:\n",
    "    print(row[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff92e2-a112-44cc-8e82-c4d2b1371cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output7L.txt\", \"Output7M.txt\", \"Output7D.txt\", \"Output7G.txt\",\n",
    "    \"Output8L.txt\", \"Output8M.txt\", \"Output8D.txt\", \"Output8G.txt\",\n",
    "    \"Output9L.txt\", \"Output9M.txt\", \"Output9D.txt\", \"Output9G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-lite\")\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "    \n",
    "# === 8. Rank-Weighted Similarity Function ===\n",
    "def rank_weighted_similarity(list1, list2):\n",
    "    \"\"\"Compute similarity score accounting for feature rank (higher ranks weigh more).\"\"\"\n",
    "    max_len = max(len(list1), len(list2))\n",
    "    score = 0.0\n",
    "    for i, f1 in enumerate(list1):\n",
    "        for j, f2 in enumerate(list2):\n",
    "            if f1 == f2:\n",
    "                # Weight: closer to top = higher weight\n",
    "                weight = 1 / (1 + abs(i - j))\n",
    "                score += weight\n",
    "    # Normalize by average length to keep 0-1 scale\n",
    "    return score / max_len\n",
    "\n",
    "# === 9. Compute Similarity Matrix ===\n",
    "file_list = sorted(normalized_outputs.keys())\n",
    "similarity_matrix = []\n",
    "\n",
    "for f1 in file_list:\n",
    "    row = [f1]\n",
    "    for f2 in file_list:\n",
    "        sim = rank_weighted_similarity(normalized_outputs[f1], normalized_outputs[f2])\n",
    "        row.append(round(sim, 3))\n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "# === 10. Save Similarity Matrix to CSV ===\n",
    "csv_path = os.path.join(folder_path, \"Feature_List_Similarity_Matrix7-9.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"File\"] + file_list)\n",
    "    writer.writerows(similarity_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved rank-weighted similarity matrix: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top Similarities ===\n",
    "print(\"\\n=== Sample Similarities ===\")\n",
    "for row in similarity_matrix[:5]:\n",
    "    print(row[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2bea74-8ae8-4bdb-b79d-9833354acc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output10L.txt\", \"Output10M.txt\", \"Output10D.txt\", \"Output10G.txt\",\n",
    "    \"Output11L.txt\", \"Output11M.txt\", \"Output11D.txt\", \"Output11G.txt\",\n",
    "    \"Output12L.txt\", \"Output12M.txt\", \"Output12D.txt\", \"Output12G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-lite\")\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "    \n",
    "# === 8. Rank-Weighted Similarity Function ===\n",
    "def rank_weighted_similarity(list1, list2):\n",
    "    \"\"\"Compute similarity score accounting for feature rank (higher ranks weigh more).\"\"\"\n",
    "    max_len = max(len(list1), len(list2))\n",
    "    score = 0.0\n",
    "    for i, f1 in enumerate(list1):\n",
    "        for j, f2 in enumerate(list2):\n",
    "            if f1 == f2:\n",
    "                # Weight: closer to top = higher weight\n",
    "                weight = 1 / (1 + abs(i - j))\n",
    "                score += weight\n",
    "    # Normalize by average length to keep 0-1 scale\n",
    "    return score / max_len\n",
    "\n",
    "# === 9. Compute Similarity Matrix ===\n",
    "file_list = sorted(normalized_outputs.keys())\n",
    "similarity_matrix = []\n",
    "\n",
    "for f1 in file_list:\n",
    "    row = [f1]\n",
    "    for f2 in file_list:\n",
    "        sim = rank_weighted_similarity(normalized_outputs[f1], normalized_outputs[f2])\n",
    "        row.append(round(sim, 3))\n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "# === 10. Save Similarity Matrix to CSV ===\n",
    "csv_path = os.path.join(folder_path, \"Feature_List_Similarity_Matrix10-12.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"File\"] + file_list)\n",
    "    writer.writerows(similarity_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved rank-weighted similarity matrix: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top Similarities ===\n",
    "print(\"\\n=== Sample Similarities ===\")\n",
    "for row in similarity_matrix[:5]:\n",
    "    print(row[:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982c69a-334d-48be-9b4a-97b1599c1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports & Configuration ===\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import statistics\n",
    "from collections import defaultdict, Counter\n",
    "import google.generativeai as genai\n",
    "\n",
    "# === 2. Configuration ===\n",
    "folder_path = r\"C:\\PhD\\Prompt_Engineering\\Step1_Local_Outputs\"\n",
    "file_names = [\n",
    "    \"Output13L.txt\", \"Output13M.txt\", \"Output13D.txt\", \"Output13G.txt\",\n",
    "    \"Output14L.txt\", \"Output14M.txt\", \"Output14D.txt\", \"Output14G.txt\",\n",
    "    \"Output15L.txt\", \"Output15M.txt\", \"Output15D.txt\", \"Output15G.txt\"\n",
    "]\n",
    "\n",
    "# === 3. Gemini API Configuration ===\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=API_KEY)  # Replace with your key\n",
    "model = genai.GenerativeModel(\"models/gemini-2.5-flash-lite\")\n",
    "\n",
    "# === 4. Load Ordered Features ===\n",
    "def load_ordered_features(folder_path, file_names):\n",
    "    outputs = {}\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è File not found: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        lines = content.splitlines()\n",
    "        features = [\n",
    "            re.sub(r\"^(\\*|-|‚Ä¢|\\d+\\.)\\s*\", \"\", ln.strip())  # remove bullet points or numbering\n",
    "            for ln in lines if re.match(r\"^(\\*|-|‚Ä¢|\\d+\\.)\", ln.strip())\n",
    "        ]\n",
    "        outputs[file_name] = features\n",
    "    return outputs\n",
    "\n",
    "llm_outputs = load_ordered_features(folder_path, file_names)\n",
    "print(f\"‚úÖ Loaded {len(llm_outputs)} files with ordered features.\")\n",
    "\n",
    "# === 5. Ask Gemini to Normalize Feature Names ===\n",
    "prompt = f\"\"\"\n",
    "You will be given multiple lists of biochemical or chemical features extracted from different models.\n",
    "\n",
    "For each unique feature string, normalize it to a single canonical concept name.\n",
    "Keep a mapping in JSON format like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"original\": \"Mol weight\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  {{\n",
    "    \"original\": \"MW\",\n",
    "    \"normalized\": \"Molecular weight\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Respond ONLY with valid JSON (a single array, no markdown or explanations).\n",
    "\n",
    "Here are the lists:\n",
    "{llm_outputs}\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Sending normalization request to Gemini (may take a minute)...\")\n",
    "response = model.generate_content(prompt)\n",
    "gemini_output = response.text.strip()\n",
    "\n",
    "# === 6. Parse Gemini JSON Output Safely ===\n",
    "def extract_json(text):\n",
    "    match = re.search(r\"(\\[.*\\])\", text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Gemini output not valid JSON.\")\n",
    "    return None\n",
    "\n",
    "mapping = extract_json(gemini_output)\n",
    "if not mapping:\n",
    "    raw_path = os.path.join(folder_path, \"Gemini_Normalization_RAW.txt\")\n",
    "    with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(gemini_output)\n",
    "    raise SystemExit(f\"‚ùå Could not parse Gemini output. Raw text saved at {raw_path}\")\n",
    "\n",
    "print(f\"‚úÖ Parsed {len(mapping)} normalized mappings from Gemini.\")\n",
    "\n",
    "# === 7. Apply Normalization ===\n",
    "norm_dict = {m[\"original\"].lower().strip(): m[\"normalized\"].strip() for m in mapping}\n",
    "\n",
    "normalized_outputs = {}\n",
    "for fname, feats in llm_outputs.items():\n",
    "    normalized_feats = [norm_dict.get(f.lower().strip(), f.strip()) for f in feats]\n",
    "    normalized_outputs[fname] = normalized_feats\n",
    "    \n",
    "# === 8. Rank-Weighted Similarity Function ===\n",
    "def rank_weighted_similarity(list1, list2):\n",
    "    \"\"\"Compute similarity score accounting for feature rank (higher ranks weigh more).\"\"\"\n",
    "    max_len = max(len(list1), len(list2))\n",
    "    score = 0.0\n",
    "    for i, f1 in enumerate(list1):\n",
    "        for j, f2 in enumerate(list2):\n",
    "            if f1 == f2:\n",
    "                # Weight: closer to top = higher weight\n",
    "                weight = 1 / (1 + abs(i - j))\n",
    "                score += weight\n",
    "    # Normalize by average length to keep 0-1 scale\n",
    "    return score / max_len\n",
    "\n",
    "# === 9. Compute Similarity Matrix ===\n",
    "file_list = sorted(normalized_outputs.keys())\n",
    "similarity_matrix = []\n",
    "\n",
    "for f1 in file_list:\n",
    "    row = [f1]\n",
    "    for f2 in file_list:\n",
    "        sim = rank_weighted_similarity(normalized_outputs[f1], normalized_outputs[f2])\n",
    "        row.append(round(sim, 3))\n",
    "    similarity_matrix.append(row)\n",
    "\n",
    "# === 10. Save Similarity Matrix to CSV ===\n",
    "csv_path = os.path.join(folder_path, \"Feature_List_Similarity_Matrix13-15.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"File\"] + file_list)\n",
    "    writer.writerows(similarity_matrix)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved rank-weighted similarity matrix: {csv_path}\")\n",
    "\n",
    "# === 11. Preview Top Similarities ===\n",
    "print(\"\\n=== Sample Similarities ===\")\n",
    "for row in similarity_matrix[:5]:\n",
    "    print(row[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce07dd57-1e4e-4460-bc02-1ba117be3403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
